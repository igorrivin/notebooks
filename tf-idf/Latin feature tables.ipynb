{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from cltk.corpus.utils.formatter import assemble_phi5_works_filepaths\n",
    "from cltk.corpus.utils.formatter import phi5_plaintext_cleanup\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from cltk.tag.pos import POSTag\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def works_texts_list(rm_punctuation, rm_periods):\n",
    "    fps = assemble_phi5_works_filepaths()\n",
    "    curly_comp = re.compile(r'{.+?}')\n",
    "    _list = []\n",
    "    for fp in fps:\n",
    "        with open(fp) as fo:\n",
    "            fr = fo.read()\n",
    "        text = phi5_plaintext_cleanup(fr, rm_punctuation, rm_periods)\n",
    "        text = curly_comp.sub('', text)\n",
    "        _list.append(text)\n",
    "    return _list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts 836\n",
      "Time to build list of texts: 81.57707095146179\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "text_list = works_texts_list(rm_punctuation=True, rm_periods=True)\n",
    "print('Total texts', len(text_list))\n",
    "print('Time to build list of texts: {}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words, indivudual word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag of words/word count\n",
    "def bow_csv():\n",
    "    t0 = time.time()\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    column_names = ['wc_' + w for w in vectorizer.get_feature_names()]\n",
    "    term_document_matrix = vectorizer.fit_transform(text_list)\n",
    "    dataframe_bow = pd.DataFrame(term_document_matrix.toarray(), columns=column_names)\n",
    "    print('DF BOW shape', dataframe_bow.shape)\n",
    "\n",
    "    fp = os.path.expanduser('~/cltk_data/user_data/bow_latin.csv')\n",
    "    dataframe_bow.to_csv(fp)\n",
    "    print('Time to create BOW vectorizer and write csv: {}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bow_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "def tfidf_csv():\n",
    "    t0 = time.time()\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    column_names = ['tfidf_' + w for w in vectorizer.get_feature_names()]\n",
    "    term_document_matrix = vectorizer.fit_transform(text_list)\n",
    "    dataframe_tfidf = pd.DataFrame(term_document_matrix.toarray(), columns=column_names)\n",
    "    print('DF tf-idf shape', dataframe_tfidf.shape)\n",
    "    \n",
    "    fp = os.path.expanduser('~/cltk_data/user_data/tfidf_latin.csv')\n",
    "    dataframe_tfidf.to_csv(fp)\n",
    "    print('Time to create tf-idf vectorizer and write csv: {}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tfidf_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Character, simple word, and sentence counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# char count\n",
    "# word count\n",
    "# sentence\n",
    "# word count lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create doc len counts: 0.24905800819396973\n"
     ]
    }
   ],
   "source": [
    "def char_len():\n",
    "    \"\"\"Count char len in an input string (doc).\"\"\"\n",
    "    t0 = time.time()\n",
    "    char_len = {}\n",
    "    for i, doc in enumerate(text_list):\n",
    "        char_len[i] = pd.Series(len(doc), index=['char_len'])\n",
    "    df_char_len = pd.DataFrame(char_len).transpose()\n",
    "\n",
    "    fp = os.path.expanduser('~/cltk_data/user_data/char_len_latin.csv')\n",
    "    df_char_len.to_csv(fp)\n",
    "    print('Time to create doc len counts: {}'.format(time.time() - t0))\n",
    "\n",
    "char_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create doc word count: 11.002326011657715\n"
     ]
    }
   ],
   "source": [
    "def word_count():\n",
    "    \"\"\"Count words in an input string (doc).\"\"\"\n",
    "    t0 = time.time()\n",
    "    p = PunktLanguageVars()\n",
    "    word_count = {}\n",
    "    for i, doc in enumerate(text_list):\n",
    "        wc_doc = len(p.word_tokenize(doc))\n",
    "        word_count[i] = pd.Series(wc_doc, index=['word_count'])\n",
    "    df_word_count = pd.DataFrame(word_count).transpose()\n",
    "\n",
    "    fp = os.path.expanduser('~/cltk_data/user_data/word_count_lens_latin.csv')\n",
    "    df_word_count.to_csv(fp)\n",
    "    print('Time to create doc word count: {}'.format(time.time() - t0))\n",
    "\n",
    "word_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_list_no_cleanup = works_texts_list(rm_punctuation=False, rm_periods=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Maecenas atavis edite regibvs. Hac ode Maecenatem adloquitur indicans alium alio studio teneri rerum, quae adpetantur uel ludicri cupiditate uel gloriae; se autem putare inter deos relatuiri, si numero lyricorum poetarum adscriptus fuerit. Maecenatem ait atauis regibus editum, quod a nobilibus Etruscorum ortus sit. Palmaqve nobilis terrarvm dominos evehit ad deos. Ambiguum, utrum nobilis deos an nobilis palma. Mobiles quirites ait referens ad uulgi leuitatem. Loquitur autem de eo, qui fauorem '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list_no_cleanup[1][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ex scriptis eorum qui ueri arbitrantur .',\n",
       " 'neque ipsi eos alii modi esse atque Amilcar dixit, ostendere possunt aliter.',\n",
       " 'antequam Barcha perierat, alii rei causa in Africam missus .',\n",
       " 'tantum bellum suscitare conari aduersarios contra bellosum genus.',\n",
       " 'qui cum is ita foedus icistis .',\n",
       " 'cum iure sine periculo bellum geri poteratur.',\n",
       " 'qui intellegunt quae fiant, dissentiuntur.',\n",
       " 'Legati quo missi sunt ueniunt, dedicant mandata.',\n",
       " 'Saguntinorum Sempronius Lilybaeo celocem in Africam mittit u']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how sent tokenizer works\n",
    "s = ' ex scriptis eorum qui ueri arbitrantur . . . neque ipsi eos alii modi esse atque Amilcar dixit, ostendere possunt aliter. antequam Barcha perierat, alii rei causa in Africam missus . . . . . . tantum bellum suscitare conari aduersarios contra bellosum genus. qui cum is ita foedus icistis . . . . . . cum iure sine periculo bellum geri poteratur. qui intellegunt quae fiant, dissentiuntur. Legati quo missi sunt ueniunt, dedicant mandata. Saguntinorum Sempronius Lilybaeo celocem in Africam mittit u'\n",
    "tokenizer = TokenizeSentence('latin')\n",
    "sent_tokens = tokenizer.tokenize_sentences(s)\n",
    "sent_tokens = [s for s in sent_tokens if len(s) > 1]  # rm '.' sents\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create doc word count: 25.400434017181396\n"
     ]
    }
   ],
   "source": [
    "def sentence_count():\n",
    "    \"\"\"Count sentence in an input string (doc).\"\"\"\n",
    "    t0 = time.time()\n",
    "    tokenizer = TokenizeSentence('latin')\n",
    "    word_count = {}\n",
    "    for i, doc in enumerate(text_list_no_cleanup):\n",
    "        sent_tokens = tokenizer.tokenize_sentences(doc)\n",
    "        wc_doc = [s for s in sent_tokens if len(s) > 1]\n",
    "        word_count[i] = pd.Series(, index=['sentence_count'])\n",
    "    df_word_count = pd.DataFrame(word_count).transpose()\n",
    "\n",
    "    fp = os.path.expanduser('~/cltk_data/user_data/sentence_count_lens_latin.csv')\n",
    "    df_word_count.to_csv(fp)\n",
    "    print('Time to create doc word count: {}'.format(time.time() - t0))\n",
    "\n",
    "sentence_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create doc word count: 18.22161316871643\n"
     ]
    }
   ],
   "source": [
    "def word_len_counts():\n",
    "    \"\"\"Count words lengths in an input string (doc).\"\"\"\n",
    "    t0 = time.time()\n",
    "    p = PunktLanguageVars()\n",
    "    word_counts = {}\n",
    "    for i, doc in enumerate(text_list_no_cleanup):\n",
    "        word_tokens = p.word_tokenize(doc)\n",
    "        list_of_counts = ['word_len_' + str(len(w)) for w in word_tokens]\n",
    "        counter_word_counts = Counter(list_of_counts)\n",
    "        word_counts[i] = pd.Series(counter_word_counts, index=counter_word_counts.keys())\n",
    "    df_word_count = pd.DataFrame(word_counts).transpose()\n",
    "\n",
    "    fp = os.path.expanduser('~/cltk_data/user_data/word_count_lens_latin.csv')\n",
    "    df_word_count.to_csv(fp)\n",
    "    print('Time to create doc word count: {}'.format(time.time() - t0))\n",
    "\n",
    "word_len_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create count of words per sentence: 35.22085189819336\n"
     ]
    }
   ],
   "source": [
    "def sentence_word_count():\n",
    "    \"\"\"Count words lengths in an input string (doc).\"\"\"\n",
    "    t0 = time.time()\n",
    "    tokenizer_sent = TokenizeSentence('latin')\n",
    "    p = PunktLanguageVars()\n",
    "    word_counts = {}\n",
    "    for i, doc in enumerate(text_list_no_cleanup):\n",
    "        list_words_per_sentence = []\n",
    "        sent_tokens = tokenizer_sent.tokenize_sentences(doc)\n",
    "        sent_tokens = [s for s in sent_tokens if len(s) > 1]\n",
    "        for sent in sent_tokens:\n",
    "            word_tokens = p.word_tokenize(sent)\n",
    "            words_in_sent = len(word_tokens)\n",
    "            list_words_per_sentence.append(words_in_sent)\n",
    "        list_of_counts = ['words_in_sent_' + str(count) for count in list_words_per_sentence]\n",
    "        counter_word_counts_per_sents = Counter(list_of_counts)\n",
    "        word_counts[i] = pd.Series(counter_word_counts_per_sents,\n",
    "                                   index=counter_word_counts_per_sents.keys())\n",
    "    df_word_count_per_sent = pd.DataFrame(word_counts).transpose()\n",
    "\n",
    "    fp = os.path.expanduser('~/cltk_data/user_data/words_per_sent_latin.csv')\n",
    "    df_word_count_per_sent.to_csv(fp)\n",
    "    print('Time to create count of words per sentence: {}'.format(time.time() - t0))\n",
    "\n",
    "sentence_word_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing doc #0\n",
      "Processing doc #1\n",
      "Processing doc #2\n",
      "Processing doc #3\n",
      "Processing doc #4\n",
      "Processing doc #5\n",
      "Processing doc #6\n",
      "Processing doc #7\n",
      "Processing doc #8\n",
      "Processing doc #9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-be90b8c73420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time to create count of words per sentence: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mpos_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-be90b8c73420>\u001b[0m in \u001b[0;36mpos_counts\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msent_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_tnt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mpos_tags_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def pos_counts(index_start=0):\n",
    "    \"\"\"Count part of speech input string (doc).\"\"\"\n",
    "    t0 = time.time()\n",
    "    tokenizer_sent = TokenizeSentence('latin')\n",
    "    pos_counts = {}\n",
    "    tagger = POSTag('latin')\n",
    "    for i, doc in enumerate(text_list_no_cleanup):\n",
    "        i += index_start\n",
    "        if i % 1 == 0:\n",
    "            print('Processing doc #{}'.format(i))\n",
    "        pos_tags_list = []\n",
    "        sent_tokens = tokenizer_sent.tokenize_sentences(doc)\n",
    "        sent_tokens = [s for s in sent_tokens if len(s) > 1]\n",
    "        for sent in sent_tokens:\n",
    "            pos_tags = tagger.tag_tnt(sent.lower())\n",
    "            pos_tags = [t[1] for t in pos_tags]\n",
    "            pos_tags_list += pos_tags\n",
    "        pos_counts_counter = Counter(pos_tags_list)\n",
    "        pos_counts[i] = pd.Series(pos_counts_counter, index=pos_counts_counter.keys())\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            break\n",
    "\n",
    "    df_pos_counts = pd.DataFrame(pos_counts).transpose()\n",
    "\n",
    "    fp = os.path.expanduser('~/cltk_data/user_data/pos_counts_latin.csv')\n",
    "    df_pos_counts.to_csv(fp)\n",
    "    print('Time to create count of words per sentence: {}'.format(time.time() - t0))\n",
    "\n",
    "pos_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
