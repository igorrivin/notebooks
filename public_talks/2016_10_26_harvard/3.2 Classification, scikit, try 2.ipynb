{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_epithet_of_author\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_id_author\n",
    "import pandas\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stream_lemmatized_files(corpus_dir):\n",
    "    # return all docs in a dir\n",
    "    user_dir = os.path.expanduser('~/cltk_data/user_data/' + corpus_dir)\n",
    "    files = os.listdir(user_dir)\n",
    "\n",
    "    for file in files:\n",
    "        filepath = os.path.join(user_dir, file)\n",
    "        with open(filepath) as fo:\n",
    "            #TODO rm words less the 3 chars long\n",
    "            yield file[3:-4], fo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1823, 5)\n",
      "... finished in 0:00:13.507331\n",
      "Number of texts: 1823\n"
     ]
    }
   ],
   "source": [
    "t0 = dt.datetime.utcnow()\n",
    "\n",
    "map_id_author = get_id_author()\n",
    "\n",
    "df = pandas.DataFrame(columns=['id', 'author' 'text', 'epithet'])\n",
    "\n",
    "for _id, text in stream_lemmatized_files('tlg_lemmatized_no_accents_no_stops'):\n",
    "    author = map_id_author[_id]\n",
    "    epithet = get_epithet_of_author(_id)\n",
    "    df = df.append({'id': _id, 'author': author, 'text': text, 'epithet': epithet}, ignore_index=True)\n",
    "    \n",
    "print(df.shape)\n",
    "print('... finished in {}'.format(dt.datetime.utcnow() - t0))\n",
    "print('Number of texts:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_list = df['text'].tolist()\n",
    "\n",
    "# make a list of short texts to drop\n",
    "# For pres, get distributions of words per doc\n",
    "short_text_drop_index = [index if len(text) > 500 else None for index, text in enumerate(text_list) ]  # ~100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = dt.datetime.utcnow()\n",
    "\n",
    "# TODO: Consdier using generator to CV http://stackoverflow.com/a/21600406\n",
    "\n",
    "# time & size counts, w/ 50 texts:\n",
    "# 0:01:15 & 202M @ ngram_range=(1, 3), min_df=2, max_features=500\n",
    "# 0:00:26 & 80M @ ngram_range=(1, 2), analyzer='word', min_df=2, max_features=5000\n",
    "# 0:00:24 & 81M @ ngram_range=(1, 2), analyzer='word', min_df=2, max_features=50000\n",
    "\n",
    "# time & size counts, w/ 1823 texts:\n",
    "# 0:01:40 & 48MB @ ngram_range=(1, 1), analyzer='word', min_df=2, max_features=500000\n",
    "#  &  @ ngram_range=(1, 2), analyzer='word', min_df=2, max_features=500000\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='word', \n",
    "                             min_df=2, max_features=500000)\n",
    "term_document_matrix = vectorizer.fit_transform(text_list)  # input is a list of strings, 1 per document\n",
    "\n",
    "vector_fp = os.path.expanduser('~/cltk_data/user_data/vectorizer_test.pickle')\n",
    "joblib.dump(vectorizer, vector_fp)\n",
    "\n",
    "print('... finished in {}'.format(dt.datetime.utcnow() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform term matrix into feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put BoW vectors into a new df\n",
    "# joblib.load(vector_fp)\n",
    "dataframe_bow = pandas.DataFrame(term_document_matrix.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids_list = df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe_bow['id'] = ids_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authors_list = df['author'].tolist()\n",
    "dataframe_bow['author'] = authors_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epithets_list = df['epithet'].tolist()\n",
    "dataframe_bow['epithet'] = epithets_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For pres, give distribution of epithets, including None\n",
    "dataframe_bow['epithet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removes 334\n",
    "#! remove rows whose epithet = None\n",
    "# note on selecting none in pandas: http://stackoverflow.com/a/24489602\n",
    "dataframe_bow = dataframe_bow[dataframe_bow.epithet.notnull()]\n",
    "dataframe_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "dataframe_bow.to_csv(os.path.expanduser('~/cltk_data/user_data/tlg_bow.csv'))\n",
    "td = (time.time() - t0) / 60\n",
    "print('Time to save csv: {} mins'.format(td))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe_bow.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = dataframe_bow['epithet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataframe_bow.drop(['epithet', 'id', 'author'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import clone\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"Take Vectors, \n",
    "    \"\"\"\n",
    "\n",
    "    '''\n",
    "    -PREPOCESSING \n",
    "    -Here, scaled data has zero mean and unit varience\n",
    "    -We save the scaler to later use with testing/prediction data\n",
    "    '''\n",
    "    print('Scaling data ...')\n",
    "    t0 = dt.datetime.utcnow()\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    fp_scaler = os.path.expanduser('~/cltk_data/user_data/tlg_bow_scaler.pickle')\n",
    "    joblib.dump(scaler, fp_scaler)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print('... finished in {} secs.'.format(dt.datetime.utcnow() - t0))\n",
    "    print()\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, Y_train, Y_test = scale_data(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_tree(X_train_scaled, X_test_scaled, Y_train, Y_test):\n",
    "    \"\"\"Run decision tree with scikit.\n",
    "    \n",
    "    Experiment with: 'max_depth'\n",
    "    \"\"\"\n",
    "    '''\n",
    "    -This is where we define the models with pre-defined parameters\n",
    "    -We can learn these parameters given our data\n",
    "    '''\n",
    "    print('Defining and fitting models ...')\n",
    "    t0 = dt.datetime.utcnow()\n",
    "    dec_tree = DecisionTreeClassifier()\n",
    "\n",
    "    dec_tree.fit(X_train_scaled, Y_train)\n",
    "\n",
    "    joblib.dump(dec_tree, 'models/tree.pickle')  #! ch to cltk_data/user_data\n",
    "\n",
    "    print('... finished in {} secs.'.format(dt.datetime.utcnow() - t0))\n",
    "    print()\n",
    "    \n",
    "\n",
    "    Y_prediction_tree = dec_tree.predict(X_test_scaled)\n",
    "    print('tree_predictions ', Y_prediction_tree)\n",
    "\n",
    "    expected = Y_test\n",
    "    print('actual_values   ', expected)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('----Tree_report--------------------------------')\n",
    "    print(classification_report(expected, Y_prediction_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_tree(X_train_scaled, X_test_scaled, Y_train, Y_test)  # 1 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_svc(X_train_scaled, X_test_scaled, Y_train, Y_test):\n",
    "    \"\"\"Run SVC with scikit.\"\"\"\n",
    "    # This is where we define the models with pre-defined parameters\n",
    "    # We can learn these parameters given our data\n",
    "    print('Defining and fitting models ...')\n",
    "    t0 = dt.datetime.utcnow()   \n",
    "    scv = svm.LinearSVC(C=100.)\n",
    "\n",
    "    scv.fit(X_train_scaled, Y_train)\n",
    "\n",
    "    joblib.dump(scv, 'models/svc.pickle')\n",
    "\n",
    "    print('... finished in {} secs.'.format(dt.datetime.utcnow() - t0))\n",
    "    print()\n",
    "    \n",
    "\n",
    "    Y_prediction_svc = scv.predict(X_test_scaled)\n",
    "    print('tree_predictions ', Y_prediction_svc)\n",
    "\n",
    "    expected = Y_test\n",
    "    print('actual_values   ', expected)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('----SVC_report--------------------------------')\n",
    "    print(classification_report(expected, Y_prediction_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_svc(X_train_scaled, X_test_scaled, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_random_forest(X_train_scaled, X_test_scaled, Y_train, Y_test):\n",
    "    \"\"\"Scikit random forest\n",
    "    \n",
    "    Experiment with 'n_estimators'\n",
    "    \"\"\"\n",
    "    \n",
    "    n_estimators = 30\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "    # Train\n",
    "    clf = clone(rf_model)\n",
    "    clf = rf_model.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "    joblib.dump(clf, 'models/random_forest.pickle')\n",
    "    \n",
    "    scores = clf.score(X_train_scaled, Y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Y_prediction = clf.predict(X_test_scaled)\n",
    "    print('tree_predictions ', Y_prediction)\n",
    "\n",
    "    expected = Y_test\n",
    "    print('actual_values   ', expected)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('----Random forest report--------------------------------')\n",
    "    print(classification_report(expected, Y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_random_forest(X_train_scaled, X_test_scaled, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_ada_boost(X_train_scaled, X_test_scaled, Y_train, Y_test):\n",
    "    \"\"\"Scikit random forest.\n",
    "    \n",
    "    For plotting see:\n",
    "    http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html\n",
    "    \n",
    "    Experiment with 'n_estimators'\n",
    "    \"\"\"\n",
    "    \n",
    "    n_estimators = 30\n",
    "    ada_classifier = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                                        n_estimators=n_estimators)\n",
    "\n",
    "    # Train\n",
    "    clf = clone(ada_classifier)\n",
    "    clf = ada_classifier.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "    joblib.dump(clf, 'models/ada_boost.pickle')\n",
    "    \n",
    "    scores = clf.score(X_train_scaled, Y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Y_prediction = clf.predict(X_test_scaled)\n",
    "    print('tree_predictions ', Y_prediction)\n",
    "\n",
    "    expected = Y_test\n",
    "    print('actual_values   ', expected)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(classification_report(expected, Y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_ada_boost(X_train_scaled, X_test_scaled, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
