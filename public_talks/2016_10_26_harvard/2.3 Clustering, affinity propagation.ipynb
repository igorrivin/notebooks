{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit Affinity propagation: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_epithet_index\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_epithets\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import select_authors_by_epithet\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_epithet_of_author\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_id_author\n",
    "from cltk.stop.greek.stops import STOPS_LIST as greek_stops\n",
    "from cltk.tokenize.word import nltk_tokenize_words\n",
    "\n",
    "from greek_accentuation.characters import base\n",
    "\n",
    "import pandas  # pip install pandas\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stream_lemmatized_files(corpus_dir, reject_none_epithet=False, reject_chars_less_than=None):\n",
    "    # return all docs in a dir; parameters for removing by None epithet and short texts\n",
    "    user_dir = os.path.expanduser('~/cltk_data/user_data/' + corpus_dir)\n",
    "    files = os.listdir(user_dir)\n",
    "    \n",
    "    map_id_author = get_id_author()\n",
    "\n",
    "    for file in files:\n",
    "        filepath = os.path.join(user_dir, file)\n",
    "        file_id = file[3:-4]\n",
    "        author = map_id_author[file_id]\n",
    "\n",
    "        if reject_none_epithet:\n",
    "            # get id numbers and then epithets of each author\n",
    "            author_epithet = get_epithet_of_author(file_id)\n",
    "            if not author_epithet:\n",
    "                continue\n",
    "\n",
    "        with open(filepath) as fo:\n",
    "            \n",
    "            text = fo.read()\n",
    "            \n",
    "            if reject_chars_less_than:\n",
    "                if len(text) < reject_chars_less_than:\n",
    "                    continue\n",
    "            \n",
    "            yield file_id, author, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... finished in 0:00:02.232875\n",
      "Number of texts: 1143\n"
     ]
    }
   ],
   "source": [
    "t0 = dt.datetime.utcnow()\n",
    "\n",
    "id_author_text_list = []\n",
    "for tlg_id, author, text in stream_lemmatized_files('tlg_lemmatized_no_accents_no_stops', \n",
    "                                    reject_none_epithet=True,\n",
    "                                    reject_chars_less_than=500):\n",
    "    id_author_text_list.append((tlg_id, author, text))\n",
    "\n",
    "print('... finished in {}'.format(dt.datetime.utcnow() - t0))\n",
    "print('Number of texts:', len(id_author_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... finished in 0:01:08.705935\n"
     ]
    }
   ],
   "source": [
    "t0 = dt.datetime.utcnow()\n",
    "\n",
    "# tf-idf features\n",
    "n_samples = 2000\n",
    "n_features = 1000  # TODO: increase\n",
    "n_topics = len(get_epithets())  # 55\n",
    "n_top_words = 20\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, \n",
    "                                   min_df=1,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=None)\n",
    "texts_list = [t[2] for t in id_author_text_list]\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts_list)\n",
    "\n",
    "# save features\n",
    "vector_fp = os.path.expanduser('~/cltk_data/user_data/tfidf_{0}features.pickle'.format(n_features))\n",
    "joblib.dump(tfidf, vector_fp)\n",
    "\n",
    "print('... finished in {}'.format(dt.datetime.utcnow() - t0))\n",
    "# time on good server:\n",
    "# 1000 features: 0:01:22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "af = AffinityPropagation(preference=-50).fit(tfidf_matrix)  #! ch preference\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "n_clusters_ = len(cluster_centers_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 1143\n"
     ]
    }
   ],
   "source": [
    "print('Estimated number of clusters: %d' % n_clusters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
