{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cross-validates the CLTK's part-of-speech taggers. The final results are found at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import TaggedCorpusReader\n",
    "from nltk.tag import AffixTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import tnt\n",
    "from nltk.tag import TrigramTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from statistics import mean\n",
    "from statistics import stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_training_set_rel = '~/latin_treebank_perseus/latin_training_set.pos'\n",
    "full_training_set = os.path.expanduser(full_training_set_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop #0\n",
      "Unigram: 0.6875379939209726\n",
      "Bigram: 0.11043566362715299\n",
      "Trigram: 0.07720364741641338\n",
      "1, 2, 3-gram backoff: 0.6942249240121581\n",
      "TnT: 0.7075987841945289\n",
      "Loop #1\n",
      "Unigram: 0.6730807777109641\n",
      "Bigram: 0.10924032872319102\n",
      "Trigram: 0.079374624173181\n",
      "1, 2, 3-gram backoff: 0.6808979755462017\n",
      "TnT: 0.6963319302465424\n",
      "Loop #2\n",
      "Unigram: 0.6664298401420959\n",
      "Bigram: 0.09218472468916519\n",
      "Trigram: 0.07158081705150977\n",
      "1, 2, 3-gram backoff: 0.672291296625222\n",
      "TnT: 0.6895204262877442\n",
      "Loop #3\n",
      "Unigram: 0.6941284403669725\n",
      "Bigram: 0.10128440366972477\n",
      "Trigram: 0.0781651376146789\n",
      "1, 2, 3-gram backoff: 0.7012844036697248\n",
      "TnT: 0.7157798165137614\n",
      "Loop #4\n",
      "Unigram: 0.6781718186990328\n",
      "Bigram: 0.09008154750616347\n",
      "Trigram: 0.06523800493077944\n",
      "1, 2, 3-gram backoff: 0.6874644414944054\n",
      "TnT: 0.7022567798217334\n",
      "Loop #5\n",
      "Unigram: 0.6814953271028037\n",
      "Bigram: 0.09719626168224299\n",
      "Trigram: 0.07009345794392523\n",
      "1, 2, 3-gram backoff: 0.6919626168224299\n",
      "TnT: 0.7050467289719626\n",
      "Loop #6\n",
      "Unigram: 0.6717732669475297\n",
      "Bigram: 0.10857908847184987\n",
      "Trigram: 0.07659900421294523\n",
      "1, 2, 3-gram backoff: 0.6746457296055152\n",
      "TnT: 0.6853695901953275\n",
      "Loop #7\n",
      "Unigram: 0.6757990867579908\n",
      "Bigram: 0.09899543378995433\n",
      "Trigram: 0.07452054794520548\n",
      "1, 2, 3-gram backoff: 0.6825570776255708\n",
      "TnT: 0.7000913242009132\n",
      "Loop #8\n",
      "Unigram: 0.6858373922817534\n",
      "Bigram: 0.11558636193330836\n",
      "Trigram: 0.08542525290370925\n",
      "1, 2, 3-gram backoff: 0.6970775571375046\n",
      "TnT: 0.7122517796927689\n",
      "Loop #9\n",
      "Unigram: 0.6787082649151615\n",
      "Bigram: 0.09797482211275314\n",
      "Trigram: 0.07206714103265828\n",
      "1, 2, 3-gram backoff: 0.6819923371647509\n",
      "TnT: 0.6942163838715563\n"
     ]
    }
   ],
   "source": [
    "unigram_accuracies = []\n",
    "bigram_accuracies = []\n",
    "trigram_accuracies = []\n",
    "backoff_accuracies = []\n",
    "tnt_accuracies = []\n",
    "\n",
    "with open(full_training_set) as f:\n",
    "    training_set_string = f.read()\n",
    "\n",
    "pos_set = training_set_string.split('\\n\\n')  # mk into a list\n",
    "\n",
    "sentence_count = len(pos_set)  # 3473\n",
    "tenth = math.ceil(int(sentence_count) / int(10))\n",
    "\n",
    "random.shuffle(pos_set)\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\n",
    "    http://stackoverflow.com/a/312464\n",
    "    \"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]\n",
    "\n",
    "# a list of 10 lists\n",
    "ten_parts = list(chunks(pos_set, tenth))  # a list of 10 lists with ~347 sentences each\n",
    "\n",
    "#for counter in list(range(10)):\n",
    "for counter, part in list(enumerate(ten_parts)):\n",
    "    # map test list to part of given loop\n",
    "    test_set = ten_parts[counter]  # or: test_set = part\n",
    "    \n",
    "    # filter out this loop's test index\n",
    "    training_set_lists = [x for x in ten_parts if x is not ten_parts[counter]]\n",
    "    \n",
    "    # next concatenate the list together into 1 file ( http://stackoverflow.com/a/952952 )\n",
    "    training_set = [item for sublist in training_set_lists for item in sublist]\n",
    "        \n",
    "    # save shuffled tests to file (as NLTK trainers expect)\n",
    "    local_dir_rel = '~/cltk_data/user_data'\n",
    "    local_dir = os.path.expanduser(local_dir_rel)\n",
    "    if not os.path.isdir(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "\n",
    "    test_path = os.path.join(local_dir, 'test_latin.pos')\n",
    "    with open(test_path, 'w') as f:\n",
    "        f.write('\\n\\n'.join(test_set))\n",
    "\n",
    "    train_path = os.path.join(local_dir, 'train_latin.pos')\n",
    "    with open(train_path, 'w') as f:\n",
    "        f.write('\\n\\n'.join(training_set))\n",
    "\n",
    "    # read POS corpora\n",
    "    train_reader = TaggedCorpusReader(local_dir, 'train_latin.pos')\n",
    "    train_sents = train_reader.tagged_sents()\n",
    "\n",
    "    test_reader = TaggedCorpusReader(local_dir, 'test_latin.pos')\n",
    "    test_sents = test_reader.tagged_sents()\n",
    "    \n",
    "    print('Loop #' + str(counter))\n",
    "    # make unigram tagger\n",
    "    unigram_tagger = UnigramTagger(train_sents)\n",
    "    # evaluate unigram tagger\n",
    "    unigram_accuracy = None\n",
    "    unigram_accuracy = unigram_tagger.evaluate(test_sents)\n",
    "    unigram_accuracies.append(unigram_accuracy)\n",
    "    print('Unigram:', unigram_accuracy)\n",
    "    \n",
    "    # make bigram tagger\n",
    "    bigram_tagger = BigramTagger(train_sents)\n",
    "    # evaluate bigram tagger\n",
    "    bigram_accuracy = None\n",
    "    bigram_accuracy = bigram_tagger.evaluate(test_sents)\n",
    "    bigram_accuracies.append(bigram_accuracy)\n",
    "    print('Bigram:', bigram_accuracy)\n",
    "    \n",
    "    # make trigram tagger\n",
    "    trigram_tagger = TrigramTagger(train_sents)\n",
    "    # evaluate trigram tagger\n",
    "    trigram_accuracy = None\n",
    "    trigram_accuracy = trigram_tagger.evaluate(test_sents)\n",
    "    trigram_accuracies.append(trigram_accuracy)\n",
    "    print('Trigram:', trigram_accuracy)\n",
    "    \n",
    "    # make 1, 2, 3-gram backoff tagger\n",
    "    tagger1 = UnigramTagger(train_sents)\n",
    "    tagger2 = BigramTagger(train_sents, backoff=tagger1)\n",
    "    tagger3 = TrigramTagger(train_sents, backoff=tagger2)\n",
    "    # evaluate trigram tagger\n",
    "    backoff_accuracy = None\n",
    "    backoff_accuracy = tagger3.evaluate(test_sents)\n",
    "    backoff_accuracies.append(backoff_accuracy)\n",
    "    print('1, 2, 3-gram backoff:', backoff_accuracy)\n",
    "    \n",
    "    # make tnt tagger\n",
    "    tnt_tagger = tnt.TnT()\n",
    "    tnt_tagger.train(train_sents)\n",
    "    # evaulate tnt tagger\n",
    "    tnt_accuracy = None\n",
    "    tnt_accuracy = tnt_tagger.evaluate(test_sents)\n",
    "    tnt_accuracies.append(tnt_accuracy)\n",
    "    print('TnT:', tnt_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_accuracies_list = []\n",
    "mean_accuracy_unigram = mean(unigram_accuracies)\n",
    "standard_deviation_unigram = stdev(unigram_accuracies)\n",
    "uni = {'unigram': {'mean': mean_accuracy_unigram, 'sd': standard_deviation_unigram}}\n",
    "final_accuracies_list.append(uni)\n",
    "\n",
    "mean_accuracy_bigram = mean(bigram_accuracies)\n",
    "standard_deviation_bigram = stdev(bigram_accuracies)\n",
    "bi = {'bigram': {'mean': mean_accuracy_bigram, 'sd': standard_deviation_bigram}}\n",
    "final_accuracies_list.append(bi)\n",
    "\n",
    "mean_accuracy_trigram = mean(trigram_accuracies)\n",
    "standard_deviation_trigram = stdev(trigram_accuracies)\n",
    "tri = {'trigram': {'mean': mean_accuracy_trigram, 'sd': standard_deviation_trigram}}\n",
    "final_accuracies_list.append(tri)\n",
    "\n",
    "mean_accuracy_backoff = mean(backoff_accuracies)\n",
    "standard_deviation_backoff = stdev(backoff_accuracies)\n",
    "back = {'1, 2, 3-gram backoff': {'mean': mean_accuracy_backoff, 'sd': standard_deviation_backoff}}\n",
    "final_accuracies_list.append(back)\n",
    "\n",
    "mean_accuracy_tnt = mean(tnt_accuracies)\n",
    "standard_deviation_tnt = stdev(tnt_accuracies)\n",
    "tnt = {'tnt': {'mean': mean_accuracy_tnt, 'sd': standard_deviation_tnt}}\n",
    "final_accuracies_list.append(tnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1, 2, 3-gram backoff</th>\n",
       "      <th>bigram</th>\n",
       "      <th>tnt</th>\n",
       "      <th>trigram</th>\n",
       "      <th>unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td> 0.686440</td>\n",
       "      <td> 0.102156</td>\n",
       "      <td> 0.700846</td>\n",
       "      <td> 0.075027</td>\n",
       "      <td> 0.679296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd</th>\n",
       "      <td> 0.009606</td>\n",
       "      <td> 0.008422</td>\n",
       "      <td> 0.009724</td>\n",
       "      <td> 0.005629</td>\n",
       "      <td> 0.008234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1, 2, 3-gram backoff    bigram       tnt   trigram   unigram\n",
       "mean              0.686440  0.102156  0.700846  0.075027  0.679296\n",
       "sd                0.009606  0.008422  0.009724  0.005629  0.008234"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict = {}\n",
    "for x in final_accuracies_list:\n",
    "    final_dict.update(x)\n",
    "\n",
    "df = pd.DataFrame(final_dict)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
