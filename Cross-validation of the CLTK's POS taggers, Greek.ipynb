{
 "metadata": {
  "name": "",
  "signature": "sha256:a0917c136c562fb565c2bdd322debcc3dd5cf1172e429ef8cc8e2d71813c3b1e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook cross-validates the CLTK's part-of-speech taggers. The final results are found at the very bottom."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus.reader import TaggedCorpusReader\n",
      "from nltk.tag import AffixTagger\n",
      "from nltk.tag import BigramTagger\n",
      "from nltk.tag import tnt\n",
      "from nltk.tag import TrigramTagger\n",
      "from nltk.tag import UnigramTagger\n",
      "from nltk.tokenize import wordpunct_tokenize\n",
      "import math\n",
      "import os\n",
      "import pandas as pd\n",
      "import random\n",
      "from statistics import mean\n",
      "from statistics import stdev"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_training_set_rel = '~/greek_treebank_perseus/pos_training_set.pos'\n",
      "full_training_set = os.path.expanduser(full_training_set_rel)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This section's code is good, but it times out in IPython.\n",
      "# Consider using it in separate scripts.\n",
      "unigram_accuracies = []\n",
      "bigram_accuracies = []\n",
      "trigram_accuracies = []\n",
      "backoff_accuracies = []\n",
      "two_prefix_accuracies = []\n",
      "three_prefix_accuracies = []\n",
      "four_prefix_accuracies = []\n",
      "two_suffix_accuracies = []\n",
      "three_suffix_accuracies = []\n",
      "four_suffix_accuracies = []\n",
      "five_suffix_accuracies = []\n",
      "six_suffix_accuracies = []\n",
      "tnt_accuracies = []\n",
      "\n",
      "with open(full_training_set) as f:\n",
      "    training_set_string = f.read()\n",
      "    pos_set = training_set_string.split('\\n\\n')  # mk into a list\n",
      "\n",
      "sentence_count = len(pos_set)\n",
      "tenth = math.ceil(int(sentence_count) / int(10))\n",
      "\n",
      "random.shuffle(pos_set)\n",
      "\n",
      "def chunks(l, n):\n",
      "    \"\"\"Yield successive n-sized chunks from l.\n",
      "    http://stackoverflow.com/a/312464\n",
      "    \"\"\"\n",
      "    for i in range(0, len(l), n):\n",
      "        yield l[i:i+n]\n",
      "\n",
      "# a list of 10 lists\n",
      "ten_parts = list(chunks(pos_set, tenth))  # a list of 10 lists with 695 sentences each\n",
      "\n",
      "#for counter in list(range(10)):\n",
      "for counter, part in list(enumerate(ten_parts)):\n",
      "    # map test list to part of given loop\n",
      "    test_set = ten_parts[counter]  # or: test_set = part\n",
      "    \n",
      "    # filter out this loop's test index\n",
      "    training_set_lists = [x for x in ten_parts if x is not ten_parts[counter]]\n",
      "    \n",
      "    # next concatenate the list together into 1 file ( http://stackoverflow.com/a/952952 )\n",
      "    training_set = [item for sublist in training_set_lists for item in sublist]\n",
      "        \n",
      "    # save shuffled tests to file\n",
      "    # there might be a way of getting \n",
      "    local_dir_rel = '~/cltk_data/user_data'\n",
      "    local_dir = os.path.expanduser(local_dir_rel)\n",
      "    if not os.path.isdir(local_dir):\n",
      "        os.makedirs(local_dir)\n",
      "\n",
      "    test_path = os.path.join(local_dir, 'test_greek.pos')\n",
      "    with open(test_path, 'w') as f:\n",
      "        f.write('\\n\\n'.join(test_set))\n",
      "\n",
      "    train_path = os.path.join(local_dir, 'train_greek.pos')\n",
      "    with open(train_path, 'w') as f:\n",
      "        f.write('\\n\\n'.join(training_set))\n",
      "\n",
      "    # read POS corpora\n",
      "    train_reader = TaggedCorpusReader(local_dir, 'train_greek.pos')\n",
      "    train_sents = train_reader.tagged_sents()\n",
      "\n",
      "    test_reader = TaggedCorpusReader(local_dir, 'test_greek.pos')\n",
      "    test_sents = test_reader.tagged_sents()\n",
      "    \n",
      "    print('Loop #' + str(counter))\n",
      "    # make unigram tagger\n",
      "    unigram_tagger = UnigramTagger(train_sents)\n",
      "    # evaluate unigram tagger\n",
      "    unigram_accuracy = None\n",
      "    unigram_accuracy = unigram_tagger.evaluate(test_sents)\n",
      "    unigram_accuracies.append(unigram_accuracy)\n",
      "    print('Unigram:', unigram_accuracy)\n",
      "    \n",
      "    # make bigram tagger\n",
      "    bigram_tagger = BigramTagger(train_sents)\n",
      "    # evaluate bigram tagger\n",
      "    bigram_accuracy = None\n",
      "    bigram_accuracy = bigram_tagger.evaluate(test_sents)\n",
      "    bigram_accuracies.append(bigram_accuracy)\n",
      "    print('Bigram:', bigram_accuracy)\n",
      "    \n",
      "    # make trigram tagger\n",
      "    trigram_tagger = TrigramTagger(train_sents)\n",
      "    # evaluate trigram tagger\n",
      "    trigram_accuracy = None\n",
      "    trigram_accuracy = trigram_tagger.evaluate(test_sents)\n",
      "    trigram_accuracies.append(trigram_accuracy)\n",
      "    print('Trigram:', trigram_accuracy)\n",
      "    \n",
      "    # make 1, 2, 3-gram backoff tagger\n",
      "    tagger1 = UnigramTagger(train_sents)\n",
      "    tagger2 = BigramTagger(train_sents, backoff=tagger1)\n",
      "    tagger3 = TrigramTagger(train_sents, backoff=tagger2)\n",
      "    # evaluate trigram tagger\n",
      "    backoff_accuracy = None\n",
      "    backoff_accuracy = tagger3.evaluate(test_sents)\n",
      "    backoff_accuracies.append(backoff_accuracy)\n",
      "    print('1, 2, 3-gram backoff:', backoff_accuracy)\n",
      "    \n",
      "    # make 2-char prefix tagger\n",
      "    two_prefix_tagger = AffixTagger(train_sents, affix_length=2)\n",
      "    # evaluate 2-char prefix tagger\n",
      "    two_prefix_accuracy = None\n",
      "    two_prefix_accuracy = two_prefix_tagger.evaluate(test_sents)\n",
      "    two_prefix_accuracies.append(two_prefix_accuracy)\n",
      "    print('2-char prefix:', two_prefix_accuracy)\n",
      "    \n",
      "    # make 3-char prefix tagger\n",
      "    three_prefix_tagger = AffixTagger(train_sents, affix_length=3)\n",
      "    # evaluate 3-char prefix tagger\n",
      "    three_prefix_accuracy = None\n",
      "    three_prefix_accuracy = three_prefix_tagger.evaluate(test_sents)\n",
      "    three_prefix_accuracies.append(three_prefix_accuracy)\n",
      "    print('3-char prefix:', three_prefix_accuracy)\n",
      "\n",
      "    # make 4-char prefix tagger\n",
      "    four_prefix_tagger = AffixTagger(train_sents, affix_length=4)\n",
      "    # evaluate 4-char prefix tagger\n",
      "    four_prefix_accuracy = None\n",
      "    four_prefix_accuracy = four_prefix_tagger.evaluate(test_sents)\n",
      "    four_prefix_accuracies.append(four_prefix_accuracy)\n",
      "    print('4-char prefix:', four_prefix_accuracy)\n",
      "\n",
      "    # make 2-char suffix tagger\n",
      "    two_suffix_tagger = AffixTagger(train_sents, affix_length=2)\n",
      "    # evaluate 2-char suffix tagger\n",
      "    two_suffix_accuracy = None\n",
      "    two_suffix_accuracy = two_suffix_tagger.evaluate(test_sents)\n",
      "    two_suffix_accuracies.append(two_suffix_accuracy)\n",
      "    print('2-char suffix:', two_suffix_accuracy)\n",
      "    \n",
      "    # make 3-char suffix tagger\n",
      "    three_suffix_tagger = AffixTagger(train_sents, affix_length=3)\n",
      "    # evaluate 3-char suffix tagger\n",
      "    three_suffix_accuracy = None\n",
      "    three_suffix_accuracy = three_suffix_tagger.evaluate(test_sents)\n",
      "    three_suffix_accuracies.append(three_suffix_accuracy)\n",
      "    print('3-char suffix:', three_suffix_accuracy)\n",
      "\n",
      "    # make 4-char suffix tagger\n",
      "    four_suffix_tagger = AffixTagger(train_sents, affix_length=4)\n",
      "    # evaluate 4-char suffix tagger\n",
      "    four_suffix_accuracy = None\n",
      "    four_suffix_accuracy = four_suffix_tagger.evaluate(test_sents)\n",
      "    four_suffix_accuracies.append(four_suffix_accuracy)\n",
      "    print('4-char suffix:', four_suffix_accuracy)\n",
      "\n",
      "    # make 5-char suffix tagger\n",
      "    five_suffix_tagger = AffixTagger(train_sents, affix_length=5)\n",
      "    # evaluate 5-char suffix tagger\n",
      "    five_suffix_accuracy = None\n",
      "    five_suffix_accuracy = five_suffix_tagger.evaluate(test_sents)\n",
      "    five_suffix_accuracies.append(five_suffix_accuracy)\n",
      "    print('5-char suffix:', five_suffix_accuracy)\n",
      "\n",
      "    # make 6-char suffix tagger\n",
      "    six_suffix_tagger = AffixTagger(train_sents, affix_length=6)\n",
      "    # evaluate 6-char suffix tagger\n",
      "    six_suffix_accuracy = None\n",
      "    six_suffix_accuracy = six_suffix_tagger.evaluate(test_sents)\n",
      "    six_suffix_accuracies.append(six_suffix_accuracy)\n",
      "    print('6-char suffix:', six_suffix_accuracy)\n",
      "    \n",
      "    # make tnt tagger\n",
      "    tnt_tagger = tnt.TnT()\n",
      "    tnt_tagger.train(train_sents)\n",
      "    # evaulate tnt tagger\n",
      "    tnt_accuracy = None\n",
      "    tnt_accuracy = tnt_tagger.evaluate(test_sents)\n",
      "    tnt_accuracies.append(tnt_accuracy)\n",
      "    print('TnT:', tnt_accuracy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code was not run in IPython, but necessary for assembling statistics.\n",
      "final_accuracies_list = []\n",
      "mean_accuracy_unigram = mean(unigram_accuracies)\n",
      "standard_deviation_unigram = stdev(unigram_accuracies)\n",
      "uni = {'unigram': {'mean': mean_accuracy_unigram, 'sd': standard_deviation_unigram}}\n",
      "final_accuracies_list.append(uni)\n",
      "\n",
      "mean_accuracy_bigram = mean(bigram_accuracies)\n",
      "standard_deviation_bigram = stdev(bigram_accuracies)\n",
      "bi = {'bigram': {'mean': mean_accuracy_bigram, 'sd': standard_deviation_bigram}}\n",
      "final_accuracies_list.append(bi)\n",
      "\n",
      "mean_accuracy_trigram = mean(trigram_accuracies)\n",
      "standard_deviation_trigram = stdev(trigram_accuracies)\n",
      "tri = {'trigram': {'mean': mean_accuracy_trigram, 'sd': standard_deviation_trigram}}\n",
      "final_accuracies_list.append(tri)\n",
      "\n",
      "mean_accuracy_backoff = mean(backoff_accuracies)\n",
      "standard_deviation_backoff = stdev(backoff_accuracies)\n",
      "back = {'1, 2, 3-gram backoff': {'mean': mean_accuracy_backoff, 'sd': standard_deviation_backoff}}\n",
      "final_accuracies_list.append(back)\n",
      "\n",
      "mean_accuracy_two_prefix = mean(two_prefix_accuracies)\n",
      "standard_deviation_two_prefix = stdev(two_prefix_accuracies)\n",
      "two_pre = {'2 prefix': {'mean': mean_accuracy_two_prefix, 'sd': standard_deviation_two_prefix}}\n",
      "final_accuracies_list.append(two_pre)\n",
      "\n",
      "mean_accuracy_three_prefix = mean(three_prefix_accuracies)\n",
      "standard_deviation_three_prefix = stdev(three_prefix_accuracies)\n",
      "three_pre = {'3 prefix': {'mean': mean_accuracy_three_prefix, 'sd': standard_deviation_three_prefix}}\n",
      "final_accuracies_list.append(three_pre)\n",
      "\n",
      "mean_accuracy_four_prefix = mean(four_prefix_accuracies)\n",
      "standard_deviation_four_prefix = stdev(four_prefix_accuracies)\n",
      "four_pre = {'4 prefix': {'mean': mean_accuracy_four_prefix, 'sd': standard_deviation_four_prefix}}\n",
      "final_accuracies_list.append(four_pre)\n",
      "\n",
      "mean_accuracy_two_suffix = mean(two_suffix_accuracies)\n",
      "standard_deviation_two_suffix = stdev(two_suffix_accuracies)\n",
      "two_suff= {'2 suffix': {'mean': mean_accuracy_two_suffix, 'sd': standard_deviation_two_suffix}}\n",
      "final_accuracies_list.append(two_suff)\n",
      "\n",
      "mean_accuracy_three_suffix = mean(three_suffix_accuracies)\n",
      "standard_deviation_three_suffix = stdev(three_suffix_accuracies)\n",
      "three_suff = {'3 suffix': {'mean': mean_accuracy_three_suffix, 'sd': standard_deviation_three_suffix}}\n",
      "final_accuracies_list.append(three_suff)\n",
      "\n",
      "mean_accuracy_four_suffix = mean(four_suffix_accuracies)\n",
      "standard_deviation_four_suffix = stdev(four_suffix_accuracies)\n",
      "four_suff = {'4 suffix': {'mean': mean_accuracy_four_suffix, 'sd': standard_deviation_four_suffix}}\n",
      "final_accuracies_list.append(four_suff)\n",
      "\n",
      "mean_accuracy_five_suffix = mean(five_suffix_accuracies)\n",
      "standard_deviation_five_suffix = stdev(five_suffix_accuracies)\n",
      "five_suff = {'5 suffix': {'mean': mean_accuracy_five_suffix, 'sd': standard_deviation_five_suffix}}\n",
      "final_accuracies_list.append(five_suff)\n",
      "\n",
      "mean_accuracy_six_suffix = mean(six_suffix_accuracies)\n",
      "standard_deviation_six_suffix = stdev(six_suffix_accuracies)\n",
      "six_suff = {'6 suffix': {'mean': mean_accuracy_six_suffix, 'sd': standard_deviation_six_suffix}}\n",
      "final_accuracies_list.append(six_suff)\n",
      "\n",
      "# tnt values for 8/10\n",
      "# [0.9553020305648972, 0.9542076240709662, 0.9564495709663806, 0.9546379227530868, 0.9518869884357039, 0.95546875, 0.9559369923006287, 0.9545558706424909]\n",
      "#In [25]: mean\n",
      "#Out[25]: 0.9548057187167692\n",
      "\n",
      "mean_accuracy_tnt = mean(tnt_accuracies)\n",
      "standard_deviation_tnt = stdev(tnt_accuracies)\n",
      "tnt = {'tnt': {'mean': mean_accuracy_tnt, 'sd': standard_deviation_tnt}}\n",
      "final_accuracies_list.append(tnt)\n",
      "\n",
      "\n",
      "# added to capture output when running as script\n",
      "'''\n",
      "with open('final_accuracies.py', 'w') as f:\n",
      "    f.write(final_dict)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Note: these values were generated from the above code, \n",
      "# but through the above code run as a script, as IPython was timing out.\n",
      "final_dict = {'trigram': {'mean': 0.7259902681754967, 'sd': 0.008456454235275485}, 'unigram': {'mean': 0.9051637750829489, 'sd': 0.0018505984564185247}, '1, 2, 3-gram backoff': {'mean': 0.952937576511621, 'sd': 0.0023986408561536845}, 'bigram': {'mean': 0.7366758737449184, 'sd': 0.007628422048101589}, 'tnt': {'mean': 0.9548057187167692, 'sd': 0.0013956692853159505}}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "final_dict = {}\n",
      "for x in final_accuracies_list:\n",
      "    final_dict.update(x)\n",
      "'''\n",
      "\n",
      "df = pd.DataFrame(final_dict)\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>1, 2, 3-gram backoff</th>\n",
        "      <th>bigram</th>\n",
        "      <th>tnt</th>\n",
        "      <th>trigram</th>\n",
        "      <th>unigram</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> 0.952938</td>\n",
        "      <td> 0.736676</td>\n",
        "      <td> 0.954806</td>\n",
        "      <td> 0.725990</td>\n",
        "      <td> 0.905164</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>sd</th>\n",
        "      <td> 0.002399</td>\n",
        "      <td> 0.007628</td>\n",
        "      <td> 0.001396</td>\n",
        "      <td> 0.008456</td>\n",
        "      <td> 0.001851</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "      1, 2, 3-gram backoff    bigram       tnt   trigram   unigram\n",
        "mean              0.952938  0.736676  0.954806  0.725990  0.905164\n",
        "sd                0.002399  0.007628  0.001396  0.008456  0.001851"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}