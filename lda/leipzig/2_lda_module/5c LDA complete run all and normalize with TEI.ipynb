{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following tutorial [\"Topic Modeling for Fun and Profit\"](http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_epithets\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_epithet_of_author\n",
    "from cltk.corpus.greek.tlg.parse_tlg_indices import get_id_author\n",
    "import gensim\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put current dir in path, for importing local module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import local module\n",
    "from lda_helpers import mk_working_dir\n",
    "from lda_helpers import working_dir\n",
    "from lda_helpers import tokenize\n",
    "from lda_helpers import iter_docs\n",
    "from lda_helpers import PREPROCESS_DEACCENT\n",
    "from lda_helpers import TOK_MIN\n",
    "from lda_helpers import TOK_MAX\n",
    "from lda_helpers import DOC_MIN\n",
    "from lda_helpers import remove_ascii\n",
    "from lda_helpers import STOPS_LIST\n",
    "from lda_helpers import no_below\n",
    "from lda_helpers import no_above\n",
    "from lda_helpers import GenerateCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enable verbose print-to-screen logging for Gensim\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where our results will go in ~/cltk_data/user_data\n",
    "mk_working_dir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tlg4102.tlg006.opp-grc1.txt ['ερμηνεία', 'διαφόρων', 'εἰσ', 'κατά', 'λουκᾶν', 'εὐαγγέλιον', 'ὅτι', 'μὲν', 'ἄλλοι', 'εὐαγγελισταὶ']\n",
      "tlg2200.tlg00518.opp-grc1.txt ['νόμος', 'τὸν', 'ξενίας', 'ἁλόντα', 'πιπράσκεσθαι', 'ἑάλω', 'ξενίας', 'δημοσθένης', 'ἔπεμψε', 'φίλιππος']\n",
      "tlg0062.tlg050.1st1K-grc1.txt ['θεων', 'εκκλησια', 'ζευς', 'μηκέτι', 'τονθορύζετε', 'θεοί', 'μηδὲ', 'κατὰ', 'γωνίας', 'συστρεφόμενοι']\n",
      "tlg2959.tlg011.opp-ger1.txt ['βραβεῖον', 'δυναστείαις', 'κατὰ', 'τὸν', 'ἰώβ', 'καὶ', 'κοπρία', 'παντὸς', 'θρόνου', 'βασιλικοῦ']\n",
      "tlg2000.tlg001.opp-grc2.txt ['ζωιον', 'και', 'τις', 'ανθρωπος', 'ἡδοναὶ', 'καὶ', 'λῦπαι', 'φόβοι', 'καὶ', 'θάρρη']\n",
      "tlg0057.tlg014.1st1K-grc1.txt ['γαληνου', 'περι', 'νευρων', 'ανατομης', 'βιβλιον', 'ὅτι', 'μὲν', 'οὐδὲν', 'τῶν', 'τοῦ']\n",
      "tlg0086.tlg042.1st1K-grc1.txt ['περι', 'υπνου', 'και', 'εγρηγορσεως', 'περὶ', 'ὕπνου', 'καὶ', 'ἐγρηγόρσεως', 'σκεπτέον', 'τίνα']\n",
      "tlg0057.tlg031.1st1K-grc1.txt ['γαληνου', 'περι', 'χρειας', 'σφυγμων', 'βιβλιον', 'τίς', 'χρεία', 'τῶν', 'σφυγμῶν', 'ἆρά']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the docs post-processing\n",
    "# Open corpus iterator\n",
    "# docs_path_rel'~/cltk_data/greek/text/tlg/plaintext/'  # for tlg\n",
    "docs_path_rel = '~/cltk_data/greek/text/greek_text_first1kgreek_plaintext/'\n",
    "docs_preprocessed = os.path.expanduser(docs_path_rel)\n",
    "stream = iter_docs(docs_preprocessed, rm_ascii=remove_ascii)\n",
    "for title, tokens in itertools.islice(iter_docs(docs_preprocessed, rm_ascii=remove_ascii), 8):\n",
    "    print(title, tokens[:10])  # print the article title and its first ten tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mk word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open corpus iterator\n",
    "doc_stream = (tokens for _, tokens in iter_docs(docs_preprocessed, rm_ascii=remove_ascii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading Dictionary object from ~/cltk_data/user_data/lda_1kgreek/gensim_dict_id2word_1kgrk_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.dict\n",
      "INFO : loaded ~/cltk_data/user_data/lda_1kgreek/gensim_dict_id2word_1kgrk_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(22820 unique tokens: ['λουκᾶν', 'ἄρχονται', 'λουκᾶς', 'προοιμίοις', 'ζαχαρίου']...)\n"
     ]
    }
   ],
   "source": [
    "# store the dictionary, for future reference\n",
    "dict_name = 'gensim_dict_id2word_1kgrk_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.dict'.format(no_below, \n",
    "                                                                                                            no_above, \n",
    "                                                                                                            TOK_MIN, \n",
    "                                                                                                            TOK_MAX, \n",
    "                                                                                                            DOC_MIN, \n",
    "                                                                                                            PREPROCESS_DEACCENT)\n",
    "dict_path = os.path.join(working_dir, dict_name)\n",
    "\n",
    "# consider doing same filtering as done in the class, then combinging count\n",
    "try:\n",
    "    id2word_map = gensim.corpora.dictionary.Dictionary.load(dict_path)\n",
    "except FileNotFoundError:\n",
    "    t0 = time.time()\n",
    "    # ~4 min on TLG corpus if rm accents; ~w min if not\n",
    "    id2word_map = gensim.corpora.Dictionary(doc_stream)\n",
    "    # this cutoff might lose too much info, we'll see\n",
    "    # ignore words that appear in less than 20 documents or more than 10% documents\n",
    "    id2word_map.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    id2word_map.save(dict_path)\n",
    "    print('Time to mk new corpus dictionary:', time.time() - t0)\n",
    "print(id2word_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mk vectors\n",
    "\n",
    "Now start again with the corpus, turning the actual words into integers from our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2469, 1), (5973, 1)]\n",
      "ποίησις\n"
     ]
    }
   ],
   "source": [
    "# Illustrate what this BoW space looks like with example doc\n",
    "doc = \"περὶ ποιητικῆς αὐτῆς τε καὶ τῶν εἰδῶν αὐτῆς, ἥν τινα δύναμιν ἕκαστον ἔχει, καὶ πῶς δεῖ συνίστασθαι τοὺς μύθους [10] εἰ μέλλει καλῶς ἕξειν ἡ ποίησις, ἔτι δὲ ἐκ πόσων καὶ ποίων ἐστὶ μορίων, ὁμοίως δὲ καὶ περὶ τῶν ἄλλων ὅσα τῆς αὐτῆς ἐστι μεθόδου, λέγωμεν ἀρξάμενοι κατὰ φύσιν πρῶτον ἀπὸ τῶν πρώτων.\"\n",
    "doc = ' '.join(simple_preprocess(doc))\n",
    "bow = id2word_map.doc2bow(tokenize(doc, rm_ascii=remove_ascii))\n",
    "print(bow)  # words both in BoW dict and doc\n",
    "print(id2word_map[bow[0][0]])  # map int back to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip_docs_at = 25 # None for final\n",
    "# make the BoW corpus\n",
    "# creates a stream of bag-of-words vectors\n",
    "corpus_bow_tlg = GenerateCorpus(docs_preprocessed, id2word_map, clip_docs=clip_docs_at)\n",
    "\n",
    "# reduce corpus size for faster testing\n",
    "#corpus_bow_tlg = gensim.utils.ClippedCorpus(corpus_bow_tlg, 100)\n",
    "\n",
    "# vector = next(iter(corpus_bow_tlg))\n",
    "# print(vector)  # print the first vector in the stream\n",
    "# [(0, 1), (1, 1), (2, 1), ...]\n",
    "\n",
    "# # what is the most common word in that first article?\n",
    "# most_index, most_count = max(vector, key=lambda _tuple: _tuple[1])\n",
    "# print(id2word_map[most_index], most_count)  # μιλησιοις 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_bow_1kgrk_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_bow_1kgrk_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x22128 matrix, density=4.745% (26247/553200)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_bow_1kgrk_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to save BoW space: 4.633636236190796\n"
     ]
    }
   ],
   "source": [
    "# Save BoW\n",
    "# ~4 min on TLG corpus\n",
    "bow_name = 'gensim_bow_1kgrk_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.mm'.format(no_below, \n",
    "                                                                                                no_above, \n",
    "                                                                                                TOK_MIN, \n",
    "                                                                                                TOK_MAX, \n",
    "                                                                                                DOC_MIN, \n",
    "                                                                                                PREPROCESS_DEACCENT)\n",
    "bow_path = os.path.join(working_dir, bow_name)\n",
    "t0 = time.time()\n",
    "gensim.corpora.MmCorpus.serialize(bow_path, corpus_bow_tlg)\n",
    "print('Time to save BoW space:', time.time() - t0)\n",
    "\n",
    "# Later load saved corpus with:\n",
    "# corpus_bow_tlg = gensim.corpora.MmCorpus(bow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_included_docs = len(corpus_bow_tlg.titles)  # used later for testing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quick testing using just a part of the corpus\n",
    "\n",
    "NUM_TOPICS_LIST = [2, 3, 5, 10, 25, 50, 100]\n",
    "NUM_TOPICS_LIST.append(len(get_epithets()))  # mk topics same number as traditional epithets\n",
    "NUM_TOPICS_LIST = sorted(NUM_TOPICS_LIST)\n",
    "PASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/cltk_data/user_data/lda_1kgreek/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-62bdda120a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# open permissions to working dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# sometimes necessary for notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/cltk_data/user_data/lda_1kgreek/'"
     ]
    }
   ],
   "source": [
    "# open permissions to working dir\n",
    "# sometimes necessary for notebook\n",
    "os.chmod(working_dir, 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.5\n",
      "INFO : using symmetric eta at 4.3821209465381244e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training ...\n",
      "... 2 topics and 1 passes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 2 topics, 1 passes over the supplied corpus of 25 documents, updating every 2000 documents, evaluating every ~25 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 1 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #25/25, outstanding queue size 1\n",
      "INFO : topic #0 (0.500): 0.004*\"πεπερασμένον\" + 0.003*\"ἀπείρου\" + 0.002*\"εὔδημος\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"μεταβάλλον\" + 0.002*\"προσεχῶς\" + 0.002*\"στερήσεως\" + 0.002*\"ἀπείρῳ\" + 0.001*\"κινουμένῳ\" + 0.001*\"διαιρετὸν\"\n",
      "INFO : topic #1 (0.500): 0.002*\"πεπερασμένον\" + 0.002*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.002*\"κινουμένῳ\" + 0.002*\"προσεχῶς\" + 0.002*\"εὔδημος\" + 0.001*\"νόησις\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"ἠρεμία\" + 0.001*\"διαιρετόν\"\n",
      "INFO : topic diff=1.113922, rho=1.000000\n",
      "INFO : -9.566 per-word bound, 757.7 perplexity estimate based on a held-out corpus of 25 documents with 69697 words\n",
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics2_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics2_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x2 matrix, density=74.000% (37/50)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics2_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n",
      "INFO : saving LdaState object under ./gensim_lda_model_1kgrk_numtopics2_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state, separately None\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics2_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state\n",
      "INFO : saving LdaMulticore object under ./gensim_lda_model_1kgrk_numtopics2_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_lda_model_1kgrk_numtopics2_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.expElogbeta.npy\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics2_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model\n",
      "INFO : using symmetric alpha at 0.3333333333333333\n",
      "INFO : using symmetric eta at 4.3821209465381244e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model space: 11.044687271118164\n",
      "Beginning training ...\n",
      "... 3 topics and 1 passes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 3 topics, 1 passes over the supplied corpus of 25 documents, updating every 2000 documents, evaluating every ~25 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 1 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #25/25, outstanding queue size 1\n",
      "INFO : topic #0 (0.333): 0.003*\"πεπερασμένον\" + 0.002*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.002*\"προσεχῶς\" + 0.001*\"κινουμένῳ\" + 0.001*\"αὐτοκίνητον\" + 0.001*\"νόησις\" + 0.001*\"ἀπείρῳ\" + 0.001*\"εὔδημος\" + 0.001*\"ἠρεμεῖ\"\n",
      "INFO : topic #1 (0.333): 0.003*\"πεπερασμένον\" + 0.003*\"ἀπείρου\" + 0.002*\"κινουμένῳ\" + 0.002*\"εὔδημος\" + 0.002*\"προσεχῶς\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"ἠρεμία\" + 0.002*\"μεταβάλλον\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"νόησις\"\n",
      "INFO : topic #2 (0.333): 0.004*\"πεπερασμένον\" + 0.003*\"μεταβάλλον\" + 0.003*\"ἀπείρου\" + 0.002*\"εὔδημος\" + 0.002*\"κινουμένῳ\" + 0.002*\"προσεχῶς\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"στερήσεως\" + 0.001*\"αὐτοκίνητον\" + 0.001*\"νόησις\"\n",
      "INFO : topic diff=1.422886, rho=1.000000\n",
      "INFO : -9.778 per-word bound, 877.9 perplexity estimate based on a held-out corpus of 25 documents with 69697 words\n",
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics3_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics3_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x3 matrix, density=62.667% (47/75)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics3_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n",
      "INFO : saving LdaState object under ./gensim_lda_model_1kgrk_numtopics3_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state, separately None\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics3_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state\n",
      "INFO : saving LdaMulticore object under ./gensim_lda_model_1kgrk_numtopics3_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_lda_model_1kgrk_numtopics3_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.expElogbeta.npy\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics3_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model\n",
      "INFO : using symmetric alpha at 0.2\n",
      "INFO : using symmetric eta at 4.3821209465381244e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model space: 9.511889219284058\n",
      "Beginning training ...\n",
      "... 5 topics and 1 passes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 5 topics, 1 passes over the supplied corpus of 25 documents, updating every 2000 documents, evaluating every ~25 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 1 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #25/25, outstanding queue size 1\n",
      "INFO : topic #0 (0.200): 0.003*\"πεπερασμένον\" + 0.002*\"μεταβάλλον\" + 0.002*\"ἀπείρου\" + 0.002*\"εὔδημος\" + 0.002*\"κινουμένῳ\" + 0.002*\"αὐτοκίνητον\" + 0.001*\"διαιρετὸν\" + 0.001*\"προσεχῶς\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"νόησις\"\n",
      "INFO : topic #1 (0.200): 0.002*\"νόησις\" + 0.002*\"ἀπείρου\" + 0.002*\"πεπερασμένον\" + 0.001*\"μεταβάλλον\" + 0.001*\"προσεχῶς\" + 0.001*\"νόησιν\" + 0.001*\"ἀλλοιώσεως\" + 0.001*\"ταὐτομάτου\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"νοητῷ\"\n",
      "INFO : topic #2 (0.200): 0.003*\"πεπερασμένον\" + 0.002*\"μεταβάλλον\" + 0.002*\"προσεχῶς\" + 0.002*\"ἀπείρου\" + 0.002*\"εὔδημος\" + 0.002*\"κινουμένῳ\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"στερήσεως\" + 0.001*\"ἀπείρῳ\" + 0.001*\"ἐναντίωσις\"\n",
      "INFO : topic #3 (0.200): 0.004*\"πεπερασμένον\" + 0.003*\"ἀπείρου\" + 0.002*\"κινουμένῳ\" + 0.002*\"προσεχῶς\" + 0.002*\"μεταβάλλον\" + 0.002*\"εὔδημος\" + 0.002*\"στερήσεως\" + 0.002*\"νόησις\" + 0.002*\"ἠρεμεῖ\" + 0.002*\"ἠρεμία\"\n",
      "INFO : topic #4 (0.200): 0.004*\"πεπερασμένον\" + 0.003*\"ἀπείρου\" + 0.003*\"εὔδημος\" + 0.002*\"μεταβάλλον\" + 0.002*\"κινουμένῳ\" + 0.002*\"προσεχῶς\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"ἠρεμεῖ\" + 0.002*\"στερήσεως\" + 0.001*\"ἠρεμία\"\n",
      "INFO : topic diff=2.139176, rho=1.000000\n",
      "INFO : -10.101 per-word bound, 1098.3 perplexity estimate based on a held-out corpus of 25 documents with 69697 words\n",
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics5_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics5_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x5 matrix, density=36.800% (46/125)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics5_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n",
      "INFO : saving LdaState object under ./gensim_lda_model_1kgrk_numtopics5_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state, separately None\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics5_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state\n",
      "INFO : saving LdaMulticore object under ./gensim_lda_model_1kgrk_numtopics5_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_lda_model_1kgrk_numtopics5_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.expElogbeta.npy\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics5_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model\n",
      "INFO : using symmetric alpha at 0.1\n",
      "INFO : using symmetric eta at 4.3821209465381244e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model space: 9.570417404174805\n",
      "Beginning training ...\n",
      "... 10 topics and 1 passes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 10 topics, 1 passes over the supplied corpus of 25 documents, updating every 2000 documents, evaluating every ~25 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 1 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #25/25, outstanding queue size 1\n",
      "INFO : topic #4 (0.100): 0.003*\"πεπερασμένον\" + 0.002*\"νόησις\" + 0.002*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.002*\"εὔδημος\" + 0.002*\"προσεχῶς\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"κινουμένῳ\" + 0.001*\"ἠρεμία\" + 0.001*\"ἀπείρῳ\"\n",
      "INFO : topic #6 (0.100): 0.003*\"πεπερασμένον\" + 0.003*\"μεταβάλλον\" + 0.002*\"ἀπείρου\" + 0.002*\"προσεχῶς\" + 0.002*\"κινουμένῳ\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"εὔδημος\" + 0.002*\"διαιρετὸν\" + 0.002*\"ἀπείρῳ\" + 0.001*\"ἠρεμεῖ\"\n",
      "INFO : topic #7 (0.100): 0.003*\"πεπερασμένον\" + 0.002*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.001*\"εὔδημος\" + 0.001*\"νόησις\" + 0.001*\"κινουμένῳ\" + 0.001*\"αὐτοκίνητον\" + 0.001*\"προσεχῶς\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"ἀλλοιώσεως\"\n",
      "INFO : topic #5 (0.100): 0.003*\"πεπερασμένον\" + 0.003*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.002*\"εὔδημος\" + 0.002*\"κινουμένῳ\" + 0.002*\"αὐτοκίνητον\" + 0.001*\"προσεχῶς\" + 0.001*\"νόησις\" + 0.001*\"διαιρετὸν\" + 0.001*\"ἀπείρῳ\"\n",
      "INFO : topic #1 (0.100): 0.003*\"ἀπείρου\" + 0.003*\"πεπερασμένον\" + 0.002*\"μεταβάλλον\" + 0.002*\"προσεχῶς\" + 0.002*\"κινουμένῳ\" + 0.001*\"στερήσεως\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"εὔδημος\" + 0.001*\"νόησις\" + 0.001*\"αὐτοκίνητον\"\n",
      "INFO : topic diff=4.266308, rho=1.000000\n",
      "INFO : -11.118 per-word bound, 2222.6 perplexity estimate based on a held-out corpus of 25 documents with 69697 words\n",
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics10_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics10_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x10 matrix, density=20.000% (50/250)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics10_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n",
      "INFO : saving LdaState object under ./gensim_lda_model_1kgrk_numtopics10_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state, separately None\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics10_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state\n",
      "INFO : saving LdaMulticore object under ./gensim_lda_model_1kgrk_numtopics10_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_lda_model_1kgrk_numtopics10_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.expElogbeta.npy\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics10_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model\n",
      "INFO : using symmetric alpha at 0.04\n",
      "INFO : using symmetric eta at 4.3821209465381244e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model space: 11.825273036956787\n",
      "Beginning training ...\n",
      "... 25 topics and 1 passes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 25 topics, 1 passes over the supplied corpus of 25 documents, updating every 2000 documents, evaluating every ~25 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 1 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #25/25, outstanding queue size 1\n",
      "INFO : topic #8 (0.040): 0.003*\"πεπερασμένον\" + 0.002*\"μεταβάλλον\" + 0.002*\"εὔδημος\" + 0.002*\"ἀπείρου\" + 0.002*\"προσεχῶς\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"διαιρετὸν\" + 0.001*\"στερήσεως\" + 0.001*\"κινουμένῳ\" + 0.001*\"ἀπείρῳ\"\n",
      "INFO : topic #19 (0.040): 0.003*\"πεπερασμένον\" + 0.002*\"κομήτης\" + 0.002*\"μεταβάλλον\" + 0.002*\"προσεχῶς\" + 0.002*\"εὔδημος\" + 0.002*\"ἀπείρου\" + 0.002*\"ὑπέκκαυμα\" + 0.001*\"πλανωμένων\" + 0.001*\"κινουμένῳ\" + 0.001*\"ἀναθυμιάσεως\"\n",
      "INFO : topic #15 (0.040): 0.003*\"νόησις\" + 0.002*\"ἀπείρου\" + 0.002*\"πεπερασμένον\" + 0.001*\"μεταβάλλον\" + 0.001*\"νόησιν\" + 0.001*\"ἀπείρῳ\" + 0.001*\"ἠρεμεῖ\" + 0.001*\"αὐτοκίνητον\" + 0.001*\"εὔδημος\" + 0.001*\"κινουμένῳ\"\n",
      "INFO : topic #11 (0.040): 0.003*\"ἀπείρου\" + 0.003*\"πεπερασμένον\" + 0.002*\"εὔδημος\" + 0.002*\"κινουμένῳ\" + 0.002*\"μεταβάλλον\" + 0.002*\"στερήσεως\" + 0.001*\"αὐτοκίνητον\" + 0.001*\"διαιρετόν\" + 0.001*\"προσεχῶς\" + 0.001*\"ἠρεμεῖ\"\n",
      "INFO : topic #23 (0.040): 0.004*\"πεπερασμένον\" + 0.004*\"ἀπείρου\" + 0.003*\"μεταβάλλον\" + 0.003*\"εὔδημος\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"προσεχῶς\" + 0.002*\"κινουμένῳ\" + 0.002*\"στερήσεως\" + 0.002*\"ἠρεμία\" + 0.002*\"ἠρεμεῖ\"\n",
      "INFO : topic diff=11.550227, rho=1.000000\n",
      "INFO : -14.367 per-word bound, 21128.8 perplexity estimate based on a held-out corpus of 25 documents with 69697 words\n",
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics25_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics25_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x25 matrix, density=11.200% (70/625)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics25_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n",
      "INFO : saving LdaState object under ./gensim_lda_model_1kgrk_numtopics25_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state, separately None\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics25_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state\n",
      "INFO : saving LdaMulticore object under ./gensim_lda_model_1kgrk_numtopics25_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_lda_model_1kgrk_numtopics25_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.expElogbeta.npy\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics25_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model\n",
      "INFO : using symmetric alpha at 0.02\n",
      "INFO : using symmetric eta at 4.3821209465381244e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model space: 16.358034372329712\n",
      "Beginning training ...\n",
      "... 50 topics and 1 passes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 50 topics, 1 passes over the supplied corpus of 25 documents, updating every 2000 documents, evaluating every ~25 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 1 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #25/25, outstanding queue size 1\n",
      "INFO : topic #48 (0.020): 0.003*\"πεπερασμένον\" + 0.003*\"ἀπείρου\" + 0.002*\"εὔδημος\" + 0.002*\"μεταβάλλον\" + 0.002*\"κινουμένῳ\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"ἀπείρῳ\" + 0.002*\"προσεχῶς\" + 0.002*\"στερήσεως\" + 0.001*\"νόησις\"\n",
      "INFO : topic #28 (0.020): 0.004*\"πεπερασμένον\" + 0.004*\"ἀπείρου\" + 0.003*\"εὔδημος\" + 0.002*\"προσεχῶς\" + 0.002*\"κινουμένῳ\" + 0.002*\"μεταβάλλον\" + 0.002*\"νόησις\" + 0.002*\"ἠρεμεῖ\" + 0.002*\"ἠρεμία\" + 0.002*\"στερήσεως\"\n",
      "INFO : topic #26 (0.020): 0.002*\"πεπερασμένον\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"ἀπείρου\" + 0.001*\"εὔδημος\" + 0.001*\"μεταβάλλον\" + 0.001*\"ἀπείρῳ\" + 0.001*\"διαιρετὸν\" + 0.001*\"κινουμένῳ\" + 0.001*\"στερήσεως\" + 0.001*\"φλεβοτομίας\"\n",
      "INFO : topic #49 (0.020): 0.004*\"πεπερασμένον\" + 0.004*\"ἀπείρου\" + 0.002*\"ἠρεμεῖ\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"προσεχῶς\" + 0.002*\"κινουμένῳ\" + 0.002*\"εὔδημος\" + 0.002*\"μεταβάλλον\" + 0.002*\"στερήσεως\" + 0.002*\"ἠρεμία\"\n",
      "INFO : topic #47 (0.020): 0.003*\"ἀπείρου\" + 0.003*\"ἀλλʼ\" + 0.003*\"πεπερασμένον\" + 0.002*\"μεταβάλλον\" + 0.002*\"ἠρεμεῖ\" + 0.002*\"προσεχῶς\" + 0.002*\"οὐδʼ\" + 0.002*\"εὔδημος\" + 0.002*\"ἐπʼ\" + 0.002*\"λίμνην\"\n",
      "INFO : topic diff=24.855853, rho=1.000000\n",
      "INFO : -19.095 per-word bound, 559948.9 perplexity estimate based on a held-out corpus of 25 documents with 69697 words\n",
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics50_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics50_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x48 matrix, density=3.667% (44/1200)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics50_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n",
      "INFO : saving LdaState object under ./gensim_lda_model_1kgrk_numtopics50_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state, separately None\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics50_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state\n",
      "INFO : saving LdaMulticore object under ./gensim_lda_model_1kgrk_numtopics50_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_lda_model_1kgrk_numtopics50_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.expElogbeta.npy\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics50_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model\n",
      "INFO : using symmetric alpha at 0.01818181818181818\n",
      "INFO : using symmetric eta at 4.3821209465381244e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model space: 23.434708833694458\n",
      "Beginning training ...\n",
      "... 55 topics and 1 passes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 55 topics, 1 passes over the supplied corpus of 25 documents, updating every 2000 documents, evaluating every ~25 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 1 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #25/25, outstanding queue size 1\n",
      "INFO : topic #7 (0.018): 0.003*\"ἀπείρου\" + 0.002*\"νόησις\" + 0.002*\"ἀλλʼ\" + 0.002*\"πεπερασμένον\" + 0.002*\"μεταβάλλον\" + 0.002*\"εὔδημος\" + 0.001*\"κινουμένῳ\" + 0.001*\"προσεχῶς\" + 0.001*\"νοητῷ\" + 0.001*\"ἠρεμεῖ\"\n",
      "INFO : topic #32 (0.018): 0.003*\"ἀπείρου\" + 0.003*\"πεπερασμένον\" + 0.002*\"εὔδημος\" + 0.002*\"ἀπείρῳ\" + 0.002*\"προσεχῶς\" + 0.002*\"ἠρεμεῖ\" + 0.002*\"μεταβάλλον\" + 0.002*\"κινουμένῳ\" + 0.001*\"ἀδιαίρετον\" + 0.001*\"ἠρεμία\"\n",
      "INFO : topic #49 (0.018): 0.004*\"πεπερασμένον\" + 0.003*\"μεταβάλλον\" + 0.002*\"ἀπείρου\" + 0.002*\"προσεχῶς\" + 0.002*\"στερήσεως\" + 0.002*\"εὔδημος\" + 0.002*\"ἠρεμία\" + 0.002*\"κινουμένῳ\" + 0.002*\"ἀδιαίρετον\" + 0.001*\"αὐτοκίνητον\"\n",
      "INFO : topic #52 (0.018): 0.003*\"πεπερασμένον\" + 0.002*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"ἠρεμεῖ\" + 0.002*\"κινουμένῳ\" + 0.002*\"προσεχῶς\" + 0.001*\"στερήσεως\" + 0.001*\"ἀπείρῳ\" + 0.001*\"νόησις\"\n",
      "INFO : topic #39 (0.018): 0.004*\"πεπερασμένον\" + 0.003*\"ἀπείρου\" + 0.003*\"μεταβάλλον\" + 0.003*\"προσεχῶς\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"εὔδημος\" + 0.002*\"ἠρεμεῖ\" + 0.002*\"κινουμένῳ\" + 0.002*\"ἀδιαίρετον\" + 0.002*\"διαιρετὸν\"\n",
      "INFO : topic diff=27.585775, rho=1.000000\n",
      "INFO : -20.626 per-word bound, 1618685.7 perplexity estimate based on a held-out corpus of 25 documents with 69697 words\n",
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics55_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics55_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x55 matrix, density=4.727% (65/1375)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics55_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n",
      "INFO : saving LdaState object under ./gensim_lda_model_1kgrk_numtopics55_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state, separately None\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics55_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state\n",
      "INFO : saving LdaMulticore object under ./gensim_lda_model_1kgrk_numtopics55_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_lda_model_1kgrk_numtopics55_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.expElogbeta.npy\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics55_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model\n",
      "INFO : using symmetric alpha at 0.01\n",
      "INFO : using symmetric eta at 4.3821209465381244e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model space: 24.374109745025635\n",
      "Beginning training ...\n",
      "... 100 topics and 1 passes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 100 topics, 1 passes over the supplied corpus of 25 documents, updating every 2000 documents, evaluating every ~25 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : training LDA model using 1 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #25/25, outstanding queue size 1\n",
      "INFO : topic #2 (0.010): 0.004*\"πεπερασμένον\" + 0.002*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.002*\"προσεχῶς\" + 0.001*\"δημοσθένης\" + 0.001*\"εὔδημος\" + 0.001*\"ἀπείρῳ\" + 0.001*\"ὑπέκκαυμα\" + 0.001*\"κομήτης\" + 0.001*\"ἠρεμεῖ\"\n",
      "INFO : topic #34 (0.010): 0.003*\"ἀλλʼ\" + 0.002*\"φλεβοτομίας\" + 0.002*\"μεταβάλλον\" + 0.002*\"οὐδʼ\" + 0.002*\"πεπερασμένον\" + 0.002*\"λίμνην\" + 0.002*\"νόησις\" + 0.002*\"ἐπʼ\" + 0.001*\"ἀπείρου\" + 0.001*\"κινουμένῳ\"\n",
      "INFO : topic #51 (0.010): 0.006*\"πουλὺ\" + 0.004*\"τῇσι\" + 0.003*\"ὅκου\" + 0.003*\"ὁκόταν\" + 0.003*\"ὁκόσον\" + 0.003*\"ἐπὴν\" + 0.003*\"ὀδόντες\" + 0.003*\"θερμαινόμενον\" + 0.003*\"λιπαρὸν\" + 0.003*\"φύονται\"\n",
      "INFO : topic #20 (0.010): 0.004*\"πεπερασμένον\" + 0.004*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.002*\"νόησις\" + 0.002*\"προσεχῶς\" + 0.002*\"εὔδημος\" + 0.002*\"αὐτοκίνητον\" + 0.002*\"κινουμένῳ\" + 0.002*\"στερήσεως\" + 0.002*\"ἀπείρῳ\"\n",
      "INFO : topic #25 (0.010): 0.003*\"πεπερασμένον\" + 0.002*\"εὔδημος\" + 0.002*\"δικασταί\" + 0.002*\"κινουμένῳ\" + 0.002*\"ἀπείρου\" + 0.002*\"μεταβάλλον\" + 0.002*\"ἠρεμεῖ\" + 0.001*\"προσεχῶς\" + 0.001*\"αὐτοκίνητον\" + 0.001*\"στερήσεως\"\n",
      "INFO : topic diff=52.837652, rho=1.000000\n",
      "INFO : -29.826 per-word bound, 951859347.1 perplexity estimate based on a held-out corpus of 25 documents with 69697 words\n",
      "INFO : storing corpus in Matrix Market format to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics100_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : saving sparse matrix to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics100_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 25x99 matrix, density=3.677% (91/2475)\n",
      "INFO : saving MmCorpus index to ~/cltk_data/user_data/lda_1kgreek/gensim_lda_space_1kgrk_numtopics100_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.mm.index\n",
      "INFO : saving LdaState object under ./gensim_lda_model_1kgrk_numtopics100_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state, separately None\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics100_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.state\n",
      "INFO : saving LdaMulticore object under ./gensim_lda_model_1kgrk_numtopics100_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to ./gensim_lda_model_1kgrk_numtopics100_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model.expElogbeta.npy\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./gensim_lda_model_1kgrk_numtopics100_numpasses1_nobelow20_noabove0.1_tokmin3_tokmax20_docmin50_deaccentFalse.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model space: 36.92840552330017\n"
     ]
    }
   ],
   "source": [
    "for num_topics in NUM_TOPICS_LIST:\n",
    "    print('Beginning training ...')\n",
    "    print('... {} topics and {} passes ...'.format(num_topics, PASSES))\n",
    "    t0 = time.time()\n",
    "    lda_model = gensim.models.LdaMulticore(corpus_bow_tlg, num_topics=num_topics, id2word=id2word_map, passes=PASSES)\n",
    "    \n",
    "    # save LDA vector space\n",
    "    lda_space_name = 'gensim_lda_space_1kgrk_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.mm'.format(num_topics, \n",
    "                                                                                                                                        PASSES, \n",
    "                                                                                                                                        no_below, \n",
    "                                                                                                                                        no_above, \n",
    "                                                                                                                                        TOK_MIN, \n",
    "                                                                                                                                        TOK_MAX, \n",
    "                                                                                                                                        DOC_MIN, \n",
    "                                                                                                                                        PREPROCESS_DEACCENT)\n",
    "    path_lda = os.path.join(working_dir, lda_space_name)\n",
    "    gensim.corpora.MmCorpus.serialize(path_lda, lda_model[corpus_bow_tlg])\n",
    "    \n",
    "    # save model\n",
    "    lda_model_name = 'gensim_lda_model_1kgrk_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.model'.format(num_topics, \n",
    "                                                                                                                                           PASSES, \n",
    "                                                                                                                                           no_below, \n",
    "                                                                                                                                           no_above, \n",
    "                                                                                                                                           TOK_MIN, \n",
    "                                                                                                                                           TOK_MAX, \n",
    "                                                                                                                                           DOC_MIN, \n",
    "                                                                                                                                           PREPROCESS_DEACCENT)\n",
    "    # path_lda = os.path.join(working_dir, lda_model_name)\n",
    "    path_lda = os.path.join('.', lda_model_name)\n",
    "    lda_model.save(path_lda)\n",
    "    print('Time to train LDA model space:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Examples of how to use the model\n",
    "# lda_model.print_topics(-1)  # print a few most important words for each LDA topic\n",
    "# # transform text into the bag-of-words space\n",
    "# bow_vector = id2word_map.doc2bow(tokenize(doc, rm_ascii=remove_non_ascii))\n",
    "# print([(id2word_map[id], count) for id, count in bow_vector])\n",
    "\n",
    "# # transform into LDA space\n",
    "# lda_vector = lda_model[bow_vector]\n",
    "# print(lda_vector)\n",
    "\n",
    "# # print the document's single most prominent LDA topic\n",
    "# print(lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word intrusion\n",
    "\n",
    "> For each trained topic, they take its first ten words, then substitute one of them with another, randomly chosen word (intruder!) and see whether a human can reliably tell which one it was. If so, the trained topic is topically coherent (good); if not, the topic has no discernible theme (bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_topics in NUM_TOPICS_LIST:\n",
    "    # load model\n",
    "    lda_model_name = 'gensim_lda_model_1kgrk_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.model'.format(num_topics, \n",
    "                                                                                                                                           PASSES, \n",
    "                                                                                                                                           no_below, \n",
    "                                                                                                                                           no_above, \n",
    "                                                                                                                                           TOK_MIN, \n",
    "                                                                                                                                           TOK_MAX, \n",
    "                                                                                                                                           DOC_MIN, \n",
    "                                                                                                                                           PREPROCESS_DEACCENT)\n",
    "    print('Loading model: {} ...'.format(lda_model_name))\n",
    "    print('... for word intrusion testing ...')\n",
    "    path_lda = os.path.join(working_dir, lda_model_name)\n",
    "    lda_model = gensim.models.LdaMulticore.load(path_lda)\n",
    "    \n",
    "    # select top 50 words for each of the LDA topics\n",
    "    print('Top 50 words of each LDA model:')\n",
    "    top_words = [[word for word, _ in lda_model.show_topic(topicno, topn=50)] for topicno in range(lda_model.num_topics)]\n",
    "    print(top_words)\n",
    "    print('')\n",
    "\n",
    "    # get all top 50 words in all 20 topics, as one large set\n",
    "    all_words = set(itertools.chain.from_iterable(top_words))\n",
    "    print(\"Can you spot the misplaced word in each topic?\")\n",
    "\n",
    "    # for each topic, replace a word at a different index, to make it more interesting\n",
    "    replace_index = np.random.randint(0, 10, lda_model.num_topics)\n",
    "\n",
    "    replacements = []\n",
    "    for topicno, words in enumerate(top_words):\n",
    "        other_words = all_words.difference(words)\n",
    "        replacement = np.random.choice(list(other_words))\n",
    "        replacements.append((words[replace_index[topicno]], replacement))\n",
    "        words[replace_index[topicno]] = replacement\n",
    "        print(\"%i: %s\" % (topicno, ' '.join(words[:10])))\n",
    "    \n",
    "    print(\"Actual replacements were:\")\n",
    "    print(list(enumerate(replacements)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split doc\n",
    "\n",
    "> We'll split each document into two parts, and check that 1) topics of the first half are similar to topics of the second 2) halves of different documents are mostly dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate on 1k documents **not** used in LDA training\n",
    "docs_preprocessed = os.path.expanduser('~/cltk_data/greek/text/tlg/plaintext/')\n",
    "doc_stream = (tokens for _, tokens in iter_docs(docs_preprocessed))  # generator\n",
    "test_docs = list(itertools.islice(doc_stream, 100, 200))  # ['πανυ', 'καλως', ...], [...], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intra_inter(model, test_docs, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1 = [model[id2word_map.doc2bow(tokens[: len(tokens) // 2])] for tokens in test_docs]\n",
    "    part2 = [model[id2word_map.doc2bow(tokens[len(tokens) // 2 :])] for tokens in test_docs]\n",
    "    \n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between {} random parts (lower is better):\".format(num_pairs))    \n",
    "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for num_topics in NUM_TOPICS_LIST:\n",
    "    # load model\n",
    "    lda_model_name = 'gensim_lda_model_1kgrk_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.model'.format(num_topics, \n",
    "                                                                                                                                           PASSES, \n",
    "                                                                                                                                           no_below, \n",
    "                                                                                                                                           no_above, \n",
    "                                                                                                                                           TOK_MIN, \n",
    "                                                                                                                                           TOK_MAX, \n",
    "                                                                                                                                           DOC_MIN, \n",
    "                                                                                                                                           PREPROCESS_DEACCENT)\n",
    "    print('Loading model: {} ...'.format(lda_model_name))\n",
    "    print('... for testing split document topic matching ...')\n",
    "    path_lda = os.path.join(working_dir, lda_model_name)\n",
    "    lda_model = gensim.models.LdaMulticore.load(path_lda)\n",
    "\n",
    "    print(\"LDA results:\")\n",
    "    # what should num_pairs be?\n",
    "    intra_inter(lda_model, test_docs, num_pairs=total_included_docs)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score all docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_auth_map = get_id_author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file topics for each doc\n",
    "for num_topics in NUM_TOPICS_LIST:\n",
    "    print('num topics', num_topics)\n",
    "    # load model\n",
    "    lda_model_name = 'gensim_lda_model_1kgrk_numtopics{}_numpasses{}_nobelow{}_noabove{}_tokmin{}_tokmax{}_docmin{}_deaccent{}.model'.format(num_topics, \n",
    "                                                                                                                                           PASSES, \n",
    "                                                                                                                                           no_below, \n",
    "                                                                                                                                           no_above, \n",
    "                                                                                                                                           TOK_MIN, \n",
    "                                                                                                                                           TOK_MAX, \n",
    "                                                                                                                                           DOC_MIN, \n",
    "                                                                                                                                           PREPROCESS_DEACCENT)\n",
    "    print('Loading model: {} ...'.format(lda_model_name))\n",
    "    print('... scoring topics of all documents ...')\n",
    "    path_lda = os.path.join(working_dir, lda_model_name)\n",
    "    # https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.get_document_topics\n",
    "    lda_model = gensim.models.LdaMulticore.load(path_lda)\n",
    "\n",
    "    # mk save path name\n",
    "    scores_name = lda_model_name.rstrip('.model') + '.scores'\n",
    "    scores_path = os.path.join(working_dir, scores_name)\n",
    "    doc_topics = ''\n",
    "    print('Going to write LDA scores for each file at: \"{}\"'.format(scores_path))\n",
    "    for file_name, tokens in iter_docs(docs_preprocessed):\n",
    "        # print(file_name, tokens[:10])  # print the article title and its first ten tokens\n",
    "        # print(file_name)\n",
    "        topic_distribution = str(lda_model[id2word_map.doc2bow(tokens)])\n",
    "        # print(topic_distribution)\n",
    "        \n",
    "        # convert file name to author name, and get epithet\n",
    "        # auth_id = file_name.lstrip('TLG').rstrip('.TXT')  # for TLG\n",
    "        auth_id = file_name.rstrip('.txt')  # for 1K Greek\n",
    "        auth_name = None\n",
    "        auth_epithet = None\n",
    "        # auth_name = id_auth_map[auth_id]  # for TLG\n",
    "        # auth_epithet = str(get_epithet_of_author(auth_id))  # for TLG\n",
    "        \n",
    "        doc_topics += 'file: ' + file_name + '\\n'\n",
    "        doc_topics += 'author: ' + auth_name + '\\n'\n",
    "        doc_topics += 'epithet: ' + auth_epithet + '\\n'\n",
    "        doc_topics += topic_distribution + '\\n\\n'\n",
    "    print('Wrote file to: \"{}\"'.format(scores_path))\n",
    "    with open(scores_path, 'w') as file_open:\n",
    "        file_open.write(doc_topics)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
