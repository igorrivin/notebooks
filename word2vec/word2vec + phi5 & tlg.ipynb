{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.formatter import assemble_phi5_author_filepaths\n",
    "from cltk.corpus.utils.formatter import assemble_tlg_author_filepaths\n",
    "from cltk.corpus.utils.formatter import phi5_plaintext_cleanup\n",
    "from cltk.corpus.utils.formatter import tlg_plaintext_cleanup\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.stop.greek.stops import STOPS_LIST as greek_stops\n",
    "from cltk.stop.latin.stops import STOPS_LIST as latin_stops\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from gensim.models import Word2Vec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare PHI sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentences(corpus, lemmatize=True):\n",
    "    assert corpus in ['phi5', 'tlg']\n",
    "    p = PunktLanguageVars()\n",
    "    if corpus == 'phi5':\n",
    "        lang = 'latin'\n",
    "        filepaths = assemble_phi5_author_filepaths()\n",
    "        jv = JVReplacer()\n",
    "        sent_tokenizer = TokenizeSentence('latin')\n",
    "        lemmatizer = LemmaReplacer('latin')\n",
    "        stops = latin_stops\n",
    "    elif corpus == 'tlg':\n",
    "        lang = 'greek'\n",
    "        filepaths = assemble_tlg_author_filepaths()\n",
    "        sent_tokenizer = TokenizeSentence('greek')\n",
    "        lemmatizer = LemmaReplacer('greek')\n",
    "        stops = greek_stops\n",
    "    #filepaths = filepaths[:5]  # for testing\n",
    "    sent_tokenizer = TokenizeSentence(lang)\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath) as f:\n",
    "            text_raw = f.read()\n",
    "        if corpus == 'phi5':\n",
    "            text_clean = phi5_plaintext_cleanup(text_raw)\n",
    "        elif corpus == 'tlg':\n",
    "            text_clean = tlg_plaintext_cleanup(text_raw)\n",
    "        sent_tokens_upper = sent_tokenizer.tokenize_sentences(text_clean)  # sentence tokenize\n",
    "        sent_tokens = [s.lower() for s in sent_tokens_upper]  # lowercase\n",
    "        for sent in sent_tokens:  # tokenize words in sentences\n",
    "            sent_word_tokens = []\n",
    "            sent_word_tokens = p.word_tokenize(sent)\n",
    "            if corpus == 'phi5':\n",
    "                sent_word_tokens = [jv.replace(word) for word in sent_word_tokens]\n",
    "            sent_word_tokens_new = []\n",
    "            for word in sent_word_tokens:  # remove punctuation (final period, commas, etc)\n",
    "                # begin cleanup for corpus\n",
    "                if corpus == 'phi5':\n",
    "                    if word[-1] in ['.', '“']:\n",
    "                        word_new = word[:-1]\n",
    "                        sent_word_tokens_new.append(word_new)\n",
    "                    elif word[0] =='“':\n",
    "                        word_new = word[1:]\n",
    "                        sent_word_tokens_new.append(word_new)\n",
    "                    elif word in [',', '.', ';', ':', '\"', \"'\", '?', '-', '!', '*', '[', ']', '{', '}']:\n",
    "                        continue\n",
    "                    elif word in stops:  # remove stops\n",
    "                        continue\n",
    "                    elif '˘' in word:  # rm meter\n",
    "                        continue\n",
    "                    elif 'á' in word:  # rm accents from vowels; find more graceful way of doing this\n",
    "                        word_new = word.replace('á', 'a')\n",
    "                        sent_word_tokens_new.append(word_new)\n",
    "                    elif 'é' in word:\n",
    "                        word_new = word.replace('é', 'e')\n",
    "                        sent_word_tokens_new.append(word_new)\n",
    "                    elif 'í' in word:\n",
    "                        word_new = word.replace('í', 'i')\n",
    "                        sent_word_tokens_new.append(word_new)\n",
    "                    elif 'ó' in word: #! no 'ó' found in PHI5\n",
    "                        word_new = word.replace('ó', 'o')\n",
    "                        sent_word_tokens_new.append(word_new)\n",
    "                        print('rmd vowel', word, word_new)\n",
    "                    elif 'ú' in word:\n",
    "                        word_new = word.replace('ú', 'u')\n",
    "                        sent_word_tokens_new.append(word_new)\n",
    "                    else:\n",
    "                        sent_word_tokens_new.append(word)\n",
    "                elif corpus == 'tlg':\n",
    "                    if word[-1] in ['.', '“']:\n",
    "                        word_new = word[:-1]\n",
    "                        sent_word_tokens_new.append(word_new)\n",
    "                    elif word in stops:  # remove stops\n",
    "                        continue\n",
    "                    elif word in [',', '.', ';', ':', '\"', \"'\", '?', '-', '!', '*', '[', ']', '{', '}', 'ʹ']:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sent_word_tokens_new.append(word)\n",
    "\n",
    "            sent_word_tokens_new = [w for w in sent_word_tokens_new if len(w) > 1]  # rm short words\n",
    "\n",
    "            sentence = [w for w in sent_word_tokens_new if w]  # remove any empty words (created thru above cleanup)\n",
    "            # remove any empty sentences (created thru above cleanup)\n",
    "            if sentence:\n",
    "                if lemmatize:\n",
    "                    # lemmatize sentences\n",
    "                    yield lemmatizer.lemmatize(sentence)\n",
    "                else:\n",
    "                    yield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model, Latin lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CLTK:Loading lemmata. This may take a minute.\n"
     ]
    }
   ],
   "source": [
    "phi_sentences_lemma = get_sentences('phi5')\n",
    "\n",
    "# note word2vec can take an iterator but not generator\n",
    "model = Word2Vec(sentences=list(phi_sentences_lemma), size=100, window=5, min_count=5, workers=4)\n",
    "# If you’re finished training a model (=no more updates, only querying), you can do\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.expanduser('~/cltk_data/user_data/word2vec_phi_lemma.model')\n",
    "#model.save(model_path)  # 26 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to load:\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pario2', 0.8047900199890137),\n",
       " ('plinthus', 0.5860334634780884),\n",
       " ('cymatium', 0.5752122402191162),\n",
       " ('unus', 0.574550986289978),\n",
       " ('duo', 0.5710355043411255),\n",
       " ('impages', 0.5561530590057373),\n",
       " ('interscapilium', 0.5291818380355835),\n",
       " ('divido', 0.5206570625305176),\n",
       " ('tres', 0.5156294107437134),\n",
       " ('circulus', 0.5138579607009888)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('pars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edo1', 0.6984066367149353),\n",
       " ('habeo', 0.5768795013427734),\n",
       " ('hic', 0.5717377662658691),\n",
       " ('is', 0.5663044452667236),\n",
       " ('ille', 0.5483741760253906),\n",
       " ('facio', 0.545281171798706),\n",
       " ('qui1', 0.5408424735069275),\n",
       " ('desperauerunt', 0.47374892234802246),\n",
       " ('dico2', 0.4732477068901062),\n",
       " ('video', 0.4655439257621765)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('sum1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model, Greek lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CLTK:Loading lemmata. This may take a minute.\n"
     ]
    }
   ],
   "source": [
    "tlg_sentences_lemma = get_sentences('tlg')\n",
    "\n",
    "model = Word2Vec(sentences=list(tlg_sentences_lemma), size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.expanduser('~/cltk_data/user_data/word2vec_tlg_lemma.model')  # 64M\n",
    "#model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to load:\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ἐστὶν', 0.6621981859207153),\n",
       " ('ἐστὶ', 0.633944034576416),\n",
       " ('ἐστίν', 0.5510233044624329),\n",
       " ('ἀπετελέσθη', 0.5382561087608337),\n",
       " ('προϋπῆρχε', 0.5373127460479736),\n",
       " ('γέγονεν', 0.5287182331085205),\n",
       " ('ἐστί', 0.5228145718574524),\n",
       " ('σπαρέν', 0.5213673114776611),\n",
       " ('ὁμοφυὴς', 0.516243577003479),\n",
       " ('πέφηνεν', 0.5157825946807861)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('εἰμί')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('λαμβάνει', 0.7046980857849121),\n",
       " ('δίδωμι', 0.6249945163726807),\n",
       " ('λαμβάνειν', 0.6200978755950928),\n",
       " ('δείκνυμι', 0.6011672019958496),\n",
       " ('λάβοι', 0.5923374891281128),\n",
       " ('λαβὼν', 0.583453893661499),\n",
       " ('λαμβάνων', 0.5456539392471313),\n",
       " ('ἀναιρέω', 0.5438410043716431),\n",
       " ('οὐκέτι', 0.5282769203186035),\n",
       " ('ἀπέδωκεν', 0.5212222337722778)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('λαμβάνω')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('σύνδεσμέω', 0.6962194442749023),\n",
       " ('διαπλανάω', 0.648327112197876),\n",
       " ('μερικωτέραις', 0.6285176873207092),\n",
       " ('ὀρεγόμεναι', 0.6168799996376038),\n",
       " ('χρήσεσι', 0.6121833324432373),\n",
       " ('εἰληχόσι', 0.6054438948631287),\n",
       " ('ἰδιάζουσι', 0.6051292419433594),\n",
       " ('συνδέδεται', 0.6021724939346313),\n",
       " ('στερούμενα', 0.600803017616272),\n",
       " ('ἀποτείνονται', 0.6003795862197876)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('συνδέω')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('εὐεργετέω', 0.6935954093933105),\n",
       " ('οἰκτείρειν', 0.6903550624847412),\n",
       " ('εὔσπλαγχνος', 0.6548620462417603),\n",
       " ('ὑβρίζοντας', 0.6433862447738647),\n",
       " ('κατοικτείρει', 0.6393857002258301),\n",
       " ('ἁμαρτάνοντας', 0.6389428377151489),\n",
       " ('ἐλεήμονας', 0.6382932662963867),\n",
       " ('ὁμοδούλους', 0.6350147724151611),\n",
       " ('ἐχθραίνοντας', 0.632503092288971),\n",
       " ('μισέω', 0.6316449642181396)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('ἐλεάω')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
