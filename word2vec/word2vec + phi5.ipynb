{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.formatter import assemble_phi5_author_filepaths\n",
    "from cltk.corpus.utils.formatter import phi5_plaintext_cleanup\n",
    "from cltk.stop.latin.stops import STOPS_LIST\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare PHI sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepaths = assemble_phi5_author_filepaths()\n",
    "sent_tokenizer = TokenizeSentence('latin')\n",
    "p = PunktLanguageVars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_sentences = []\n",
    "for filepath in filepaths:\n",
    "    with open(filepath) as f:\n",
    "        text_raw = f.read()\n",
    "    text_clean = phi5_plaintext_cleanup(text_raw)  # phi5_plaintext_cleanup()\n",
    "    sent_tokens_upper = sent_tokenizer.tokenize_sentences(text_clean)  # sentence tokenize\n",
    "    sent_tokens = [s.lower() for s in sent_tokens_upper]  # lowercase\n",
    "    #sentence_tokens_author = []\n",
    "    for sent in sent_tokens:  # tokenize words in sentences\n",
    "        sent_word_tokens = []\n",
    "        sent_word_tokens = p.word_tokenize(sent)\n",
    "        sent_word_tokens_new = []\n",
    "        for word in sent_word_tokens:  # remove punctuation (final period, commas, etc)\n",
    "            if word[-1] in ['.', '“']:\n",
    "                word_new = word[:-1]\n",
    "                sent_word_tokens_new.append(word_new)\n",
    "            elif word[0] =='“':\n",
    "                word_new = word[1:]\n",
    "                sent_word_tokens_new.append(word_new)\n",
    "            elif word in [',', '.', ';', ':', '\"', \"'\", '?', '-', '!', '*', '[', ']', '{', '}']:\n",
    "                continue\n",
    "            elif word in STOPS_LIST:  # remove stops\n",
    "                continue\n",
    "            elif '˘' in word:  # rm meter\n",
    "                continue\n",
    "            elif 'á' in word:  # rm accents from vowels; find more graceful way of doing this\n",
    "                word_new = word.replace('á', 'a')\n",
    "                sent_word_tokens_new.append(word_new)\n",
    "            elif 'é' in word:\n",
    "                word_new = word.replace('é', 'e')\n",
    "                sent_word_tokens_new.append(word_new)\n",
    "            elif 'í' in word:\n",
    "                word_new = word.replace('í', 'i')\n",
    "                sent_word_tokens_new.append(word_new)\n",
    "            elif 'ó' in word: #! no 'ó' found in PHI5\n",
    "                word_new = word.replace('ó', 'o')\n",
    "                sent_word_tokens_new.append(word_new)\n",
    "                print('rmd vowel', word, word_new)\n",
    "            elif 'ú' in word:\n",
    "                word_new = word.replace('ú', 'u')\n",
    "                sent_word_tokens_new.append(word_new)\n",
    "            else:\n",
    "                sent_word_tokens_new.append(word)\n",
    "        sent_word_tokens_new = [w for w in sent_word_tokens_new if len(w) > 1]  # rm short words\n",
    "\n",
    "        sentence = [w for w in sent_word_tokens_new if w]  # remove any empty words (created thru above cleanup)\n",
    "        if sentence:  # remove any empty sentences (created thru above cleanup)\n",
    "            phi_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['calata', 'comitia'], ['curiata'], ['centuriata', 'uniuersum', 'populum', 'partem', 'aliquam', 'adesse', 'iubet', 'comitia', 'concilium', 'edicere', 'debet'], ['tribuni', 'aduocant', 'patricios', 'eos', 'referre', 'ulla', 're', 'possunt'], ['leges', 'proprie', 'plebis', 'scita', 'appellantur', 'tribunis', 'plebis', 'ferentibus', 'accepta', 'sunt']]\n",
      "Total sentences: 483762\n"
     ]
    }
   ],
   "source": [
    "print(phi_sentences[:5])\n",
    "print('Total sentences:', len(phi_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=phi_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "# If you’re finished training a model (=no more updates, only querying), you can do\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_path = os.path.expanduser('~/cltk_data/user_data/word2vec_phi.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(model_path)  # 84 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to load:\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coniugis', 0.7045540809631348),\n",
       " ('inmemor', 0.6948878765106201),\n",
       " ('exequias', 0.6811513304710388),\n",
       " ('perfide', 0.6685100197792053),\n",
       " ('parentis', 0.6603140830993652),\n",
       " (\"'quaenam\", 0.657184898853302),\n",
       " ('miserere', 0.6534832715988159),\n",
       " ('genetrix', 0.6532148122787476),\n",
       " ('miserae', 0.6508104801177979),\n",
       " ('titulum', 0.6492555141448975)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('memor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('odit', 0.7035549283027649),\n",
       " ('dolet', 0.6746412515640259),\n",
       " ('flet', 0.6656931638717651),\n",
       " ('optat', 0.6644587516784668),\n",
       " ('demens', 0.649543285369873),\n",
       " ('durus', 0.6433272361755371),\n",
       " ('risit', 0.6408871412277222),\n",
       " ('admovet', 0.6402783393859863),\n",
       " ('infelix', 0.6373776197433472),\n",
       " ('care', 0.630918025970459)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('amat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('achillis', 0.7200091481208801),\n",
       " ('troia', 0.6933267712593079),\n",
       " ('achilles', 0.690603494644165),\n",
       " ('mars', 0.6878527402877808),\n",
       " ('aeneae', 0.6771496534347534),\n",
       " ('priami', 0.6710656881332397),\n",
       " ('iove', 0.6604468822479248),\n",
       " ('hector', 0.6573183536529541),\n",
       " ('saturnia', 0.6531522870063782),\n",
       " ('senior', 0.6517125368118286)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('aeneas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gloria', 0.7287247776985168),\n",
       " ('fama', 0.6642335057258606),\n",
       " ('senectus', 0.6609640121459961),\n",
       " ('laus', 0.657171905040741),\n",
       " ('fiducia', 0.6522766351699829),\n",
       " ('lascivia', 0.6502325534820557),\n",
       " ('potentia', 0.6471846699714661),\n",
       " ('favor', 0.6339759826660156),\n",
       " ('quies', 0.6272529363632202),\n",
       " ('reverentia', 0.6216776371002197)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('felix', 0.6785154938697815),\n",
       " ('parens', 0.6661562919616699),\n",
       " ('quirine', 0.6641063690185547)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"puer\" is to \"pater\" as \"filia\" is to ...?\n",
    "model.most_similar(['filia', 'pater'], ['puer'], topn=3)  # 'should' be mater!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'canis'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which word doesn't go with the others?\n",
    "model.doesnt_match(\"filius pater mater canis\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60847165245365753"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('pater', 'mater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.092073954706546265"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('pater', 'canis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05744257,  0.22592203,  0.04018604, -0.07586946,  0.0119054 ,\n",
       "        0.10377435,  0.01969249, -0.01103721,  0.30429131,  0.0545937 ,\n",
       "       -0.25224891, -0.2993449 ,  0.00218131,  0.00991641, -0.03970297,\n",
       "        0.09024904,  0.08637553,  0.1639163 ,  0.15967998,  0.1670309 ,\n",
       "       -0.04222492, -0.14281853,  0.10452943, -0.30463064,  0.13105001,\n",
       "       -0.04037181, -0.09128801,  0.24211241,  0.03005035, -0.11448155,\n",
       "       -0.01809427,  0.06177646, -0.17334674,  0.19290391,  0.0890111 ,\n",
       "       -0.11700562,  0.20461507, -0.02512585, -0.07106511, -0.13127086,\n",
       "        0.17325069,  0.16714285, -0.05994416,  0.18736716,  0.14231586,\n",
       "        0.12930287, -0.17272429,  0.05862473,  0.26044106, -0.12149894,\n",
       "       -0.16043539, -0.19315961,  0.10559075,  0.02609053,  0.41204444,\n",
       "       -0.40747839, -0.14203864, -0.22034764, -0.18120967,  0.17194615,\n",
       "        0.04295829,  0.02291438,  0.20852986,  0.23511888,  0.21593477,\n",
       "        0.05037568, -0.02951043,  0.02491214, -0.01039343,  0.15004307,\n",
       "       -0.07241164,  0.2227497 ,  0.21617855, -0.08113196,  0.06834389,\n",
       "       -0.00569904, -0.2698507 ,  0.14832841,  0.29609945,  0.15997805,\n",
       "        0.01945854,  0.27701685, -0.20912355, -0.05700139,  0.03779659,\n",
       "       -0.32126781, -0.22996877,  0.02166177,  0.00516871, -0.0657784 ,\n",
       "       -0.09869225,  0.11653145, -0.03409592, -0.10866409, -0.13455081,\n",
       "       -0.12297951, -0.12116989, -0.03195545, -0.23835574, -0.0141314 ], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['hasta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
